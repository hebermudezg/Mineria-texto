0
sugerencias
buscar
``
www.eleconomista.es
''
accede
cuenta
¿has
olvidado
contraseña
¿no
cuenta
crear
cuenta
¿qué
és
big
data
sirve
preguntas
habituales
acercamos
tecnología
introducción
big
data
esperamos
sacar
dudas
big
data
nació
objetivo
cubrir
unas
necesidades
satisfechas
tecnologías
existentes
almacenamiento
tratamiento
grandes
volúmenes
datos
poseen
unas
características
concretas
definidas
tres
v
’
s
puede
haber
característica
importante
acerca
datos
considerados
fuente
verdad
decir
alteran
tratamiento
tecnología
subyacente
big
data
apache
hadoop
actualidad
cuenta
ocho
años
historia
¿pero
hadoop
hadoop
sistema
operativo
distribuido
permite
procesar
paralelo
grandes
volúmenes
datos
hardware
convencional
tipo
sistema
operativo
especial
trabaja
linux
windows
utilizando
implementación
hortonworks
posee
siguientes
características
pregunta
siempre
asalta
principio
¿es
si
suficientes
bases
datos
relacionales
rdbms
respuesta
hacen
tareas
diferentes
excluye
complementarias
hadoop
cubre
área
importante
tratamiento
datos
estructurados
normalmente
almacenados
bases
datos
convencionales
estudios
cifran
95
datos
producidos
empresa
probablemente
aparecen
dudas
plantearnos
quién
va
dirigida
tecnología
si
pensamos
grandes
volúmenes
datos
existen
gran
número
empresas
retirado
histórico
datos
inviable
tratamiento
tecnologías
convencionales
banca
seguros
centros
investigación
aparecen
nuevas
necesidades
derivadas
tratamiento
datos
asociados
redes
sociales
muchas
empresas
realizado
importante
inversión
hecho
abre
mercado
big
data
amplio
abanico
empresas
actualidad
realizan
análisis
datos
ello
pierden
oportunidad
mejorar
generar
nuevas
líneas
negocio
principio
parecía
tecnología
dirigida
empresas
altísimos
volúmenes
datos
va
asentándose
idea
puede
aplicarse
pequeñas
medianas
empresas
unas
necesidades
concretas
cuenta
actualmente
ritmo
crecimiento
datos
exponencial
manera
lógico
pensar
nuevas
herramientas
ayuden
tratamiento
silos
información
pueden
provenir
múltiples
diferentes
canales
plantearnos
necesidad
llevar
cabo
implantación
big
data
debemos
tener
cuenta
lado
dispondremos
solución
datos
estructurados
rdbms
convencional
estructurados
semi
hadoop
necesitaremos
dar
respuesta
análisis
datos
ello
ecosistema
big
data
dispone
múltiples
soluciones
bajo
nivel
abstraen
complejidad
necesitaremos
responder
así
termina
introducción
big
data
próximos
artículos
abordaré
ecosistema
big
data
conocer
herramientas
disponibles
aplicación
vimos
artículo
anterior
mongodb
permite
configurarlo
fácilmente
actuar
modo
replicación
replicación
mongo
da
soporte
alta
disponibilidad
necesitamos
sistemas
big
data
limitaciones
tal
explicamos
podemos
complementar
replicación
sharding
particionado
información
manera
podemos
…
hora
crear
proyectos
big
data
detecten
consuman
gestionen
organicen
presenten
dichos
datos
manera
optimizada
forma
aporten
negocio
generalmente
enfrentamos
siguientes
preguntas
¿de
dónde
obtengo
datos
¿qué
datos
aportan
información
negocio
¿qué
datos
disponibles
…
carlos
mota
explica
techtuesday
netmind
big
data
hortonworks
charla
distribución
hortonworks
muestran
novedades
hadoop
2
habla
ecosistema
big
data
hadoop
experto
explica
aplicaciones
tendencias
actuales
api
’
s
destacadas
así
certificaciones
…
sistemas
big
data
conjunto
características
hacen
únicos
volumen
volumen
datos
gran
tamaño
demasiado
grandes
caber
memoria
principal
si
indexado
índices
pueden
ser
demasiado
grandes
memoria
principal
velocidad
datos
generan
llegan
rápido
deben
…
``
big
data
''
is
field
that
treats
ways
to
analyze
systematically
extract
information
from
or
otherwise
deal
with
data
sets
that
are
too
large
or
complex
to
be
dealt
with
by
traditional
data-processing
application
software
data
with
many
cases
rows
offer
greater
statistical
power
while
data
with
higher
complexity
more
attributes
or
columns
may
lead
to
higher
false
discovery
rate
2
big
data
challenges
include
capturing
data
data
storage
data
analysis
search
sharing
transfer
visualization
querying
updating
information
privacy
and
data
source
big
data
was
originally
associated
with
three
key
concepts
volume
variety
and
velocity
3
other
concepts
later
attributed
to
big
data
are
veracity
i.e.
how
much
noise
is
in
the
data
4
and
value
5
current
usage
of
the
term
big
data
tends
to
refer
to
the
use
of
predictive
analytics
user
behavior
analytics
or
certain
other
advanced
data
analytics
methods
that
extract
value
from
data
and
seldom
to
particular
size
of
data
set
``
there
is
little
doubt
that
the
quantities
of
data
now
available
are
indeed
large
but
that
's
not
the
most
relevant
characteristic
of
this
new
data
ecosystem
``
6
analysis
of
data
sets
can
find
new
correlations
to
``
spot
business
trends
prevent
diseases
combat
crime
and
so
on
``
7
scientists
business
executives
practitioners
of
medicine
advertising
and
governments
alike
regularly
meet
difficulties
with
large
data-sets
in
areas
including
internet
searches
fintech
urban
informatics
and
business
informatics
scientists
encounter
limitations
in
e-science
work
including
meteorology
genomics
8
connectomics
complex
physics
simulations
biology
and
environmental
research
9
data
sets
grow
rapidly
in
part
because
they
are
increasingly
gathered
by
cheap
and
numerous
information-sensing
internet
of
things
devices
such
as
mobile
devices
aerial
remote
sensing
software
logs
cameras
microphones
radio-frequency
identification
rfid
readers
and
wireless
sensor
networks
10
11
the
world
's
technological
per-capita
capacity
to
store
information
roughly
doubled
every
40
months
since
the
1980s
12
as
of
2012
update
every
day
2.5
exabytes
2.5×1018
of
data
are
generated
13
based
on
an
idc
report
prediction
the
global
data
volume
will
grow
exponentially
from
4.4
zettabytes
to
44
zettabytes
between
2013
and
2020
14
by
2025
idc
predicts
there
will
be
163
zettabytes
of
data
15
one
question
for
large
enterprises
is
determining
who
should
own
big-data
initiatives
that
affect
the
entire
organization
16
relational
database
management
systems
desktop
statistics
clarification
needed
and
software
packages
used
to
visualize
data
often
have
difficulty
handling
big
data
the
work
may
require
``
massively
parallel
software
running
on
tens
hundreds
or
even
thousands
of
servers
''
17
what
qualifies
as
being
``
big
data
''
varies
depending
on
the
capabilities
of
the
users
and
their
tools
and
expanding
capabilities
make
big
data
moving
target
``
for
some
organizations
facing
hundreds
of
gigabytes
of
data
for
the
first
time
may
trigger
need
to
reconsider
data
management
options
for
others
it
may
take
tens
or
hundreds
of
terabytes
before
data
size
becomes
significant
consideration
``
18
the
term
been
in
use
since
the
1990s
with
some
giving
credit
to
john
mashey
for
popularizing
the
term
19
20
big
data
usually
includes
data
sets
with
sizes
beyond
the
ability
of
commonly
used
software
tools
to
capture
curate
manage
and
process
data
within
tolerable
elapsed
time
21
big
data
philosophy
encompasses
unstructured
semi-structured
and
structured
data
however
the
main
focus
is
on
unstructured
data
22
big
data
``
size
''
is
constantly
moving
target
as
of
2012
update
ranging
from
few
dozen
terabytes
to
many
zettabytes
of
data
23
big
data
requires
set
of
techniques
and
technologies
with
new
forms
of
integration
to
reveal
insights
from
datasets
that
are
diverse
complex
and
of
massive
scale
24
2016
definition
states
that
``
big
data
represents
the
information
assets
characterized
by
such
high
volume
velocity
and
variety
to
require
specific
technology
and
analytical
methods
for
its
transformation
into
value
''
25
similarly
kaplan
and
haenlein
define
big
data
as
``
data
sets
characterized
by
huge
amounts
volume
of
frequently
updated
data
velocity
in
various
formats
such
as
numeric
textual
or
images/videos
variety
``
26
additionally
new
v
veracity
is
added
by
some
organizations
to
describe
it
27
revisionism
challenged
by
some
industry
authorities
28
the
three
vs
volume
variety
and
velocity
been
further
expanded
to
other
complementary
characteristics
of
big
data
29
30
2018
definition
states
``
big
data
is
where
parallel
computing
tools
are
needed
to
handle
data
''
and
notes
``
this
represents
distinct
and
clearly
defined
change
in
the
computer
science
used
via
parallel
programming
theories
and
losses
of
some
of
the
guarantees
and
capabilities
made
by
codd
's
relational
model
''
33
the
growing
maturity
of
the
concept
more
starkly
delineates
the
difference
between
``
big
data
''
and
``
business
intelligence
''
34
big
data
can
be
described
by
the
following
characteristics
29
30
data
must
be
processed
with
advanced
tools
analytics
and
algorithms
to
reveal
meaningful
information
for
example
to
manage
factory
one
must
consider
both
visible
and
invisible
issues
with
various
components
information
generation
algorithms
must
detect
and
address
invisible
issues
such
as
machine
degradation
component
wear
etc
on
the
factory
floor
41
42
big
data
repositories
have
existed
in
many
forms
often
built
by
corporations
with
special
need
commercial
vendors
historically
offered
parallel
database
management
systems
for
big
data
beginning
in
the
1990s
for
many
years
wintercorp
published
the
largest
database
report
43
promotional
source
teradata
corporation
in
1984
marketed
the
parallel
processing
dbc
1012
system
teradata
systems
were
the
first
to
store
and
analyze
1
terabyte
of
data
in
1992.
hard
disk
drives
were
2.5
gb
in
1991
so
the
definition
of
big
data
continuously
evolves
according
to
kryder
's
law
teradata
installed
the
first
petabyte
class
rdbms
based
system
in
2007.
as
of
2017
update
there
are
few
dozen
petabyte
class
teradata
relational
databases
installed
the
largest
of
which
exceeds
50
pb
systems
up
until
2008
were
100
structured
relational
data
since
then
teradata
added
unstructured
data
types
including
xml
json
and
avro
in
2000
seisint
inc.
now
lexisnexis
group
developed
c++-based
distributed
file-sharing
framework
for
data
storage
and
query
the
system
stores
and
distributes
structured
semi-structured
and
unstructured
data
across
multiple
servers
users
can
build
queries
in
c++
dialect
called
ecl
ecl
uses
an
``
apply
schema
on
read
''
method
to
infer
the
structure
of
stored
data
when
it
is
queried
instead
of
when
it
is
stored
in
2004
lexisnexis
acquired
seisint
inc.
44
and
in
2008
acquired
choicepoint
inc.
45
and
their
high-speed
parallel
processing
platform
the
two
platforms
were
merged
into
hpcc
or
high-performance
computing
cluster
systems
and
in
2011
hpcc
was
open-sourced
under
the
apache
v2.0
license
quantcast
file
system
was
available
about
the
same
time
46
cern
and
other
physics
experiments
have
collected
big
data
sets
for
many
decades
usually
analyzed
via
high-performance
computing
supercomputers
rather
than
the
commodity
map-reduce
architectures
usually
meant
by
the
current
``
big
data
''
movement
in
2004
google
published
paper
on
process
called
mapreduce
that
uses
similar
architecture
the
mapreduce
concept
provides
parallel
processing
model
and
an
associated
implementation
was
released
to
process
huge
amounts
of
data
with
mapreduce
queries
are
split
and
distributed
across
parallel
nodes
and
processed
in
parallel
the
map
step
the
results
are
then
gathered
and
delivered
the
reduce
step
the
framework
was
very
successful
47
so
others
wanted
to
replicate
the
algorithm
therefore
an
implementation
of
the
mapreduce
framework
was
adopted
by
an
apache
open-source
project
named
hadoop
48
apache
spark
was
developed
in
2012
in
response
to
limitations
in
the
mapreduce
paradigm
as
it
adds
the
ability
to
set
up
many
operations
not
just
map
followed
by
reducing
mike2.0
is
an
open
approach
to
information
management
that
acknowledges
the
need
for
revisions
due
to
big
data
implications
identified
in
an
article
titled
``
big
data
solution
offering
''
49
the
methodology
addresses
handling
big
data
in
terms
of
useful
permutations
of
data
sources
complexity
in
interrelationships
and
difficulty
in
deleting
or
modifying
individual
records
50
2012
studies
showed
that
multiple-layer
architecture
is
one
option
to
address
the
issues
that
big
data
presents
distributed
parallel
architecture
distributes
data
across
multiple
servers
these
parallel
execution
environments
can
dramatically
improve
data
processing
speeds
this
type
of
architecture
inserts
data
into
parallel
dbms
which
implements
the
use
of
mapreduce
and
hadoop
frameworks
this
type
of
framework
looks
to
make
the
processing
power
transparent
to
the
end
user
by
using
front-end
application
server
51
the
data
lake
allows
an
organization
to
shift
its
focus
from
centralized
control
to
shared
model
to
respond
to
the
changing
dynamics
of
information
management
this
enables
quick
segregation
of
data
into
the
data
lake
thereby
reducing
the
overhead
time
52
53
big
data
analytics
for
manufacturing
applications
is
marketed
as
``
5c
architecture
''
connection
conversion
cyber
cognition
and
configuration
54
factory
work
and
cyber-physical
systems
may
have
an
extended
``
6c
system
''
2011
mckinsey
global
institute
report
characterizes
the
main
components
and
ecosystem
of
big
data
as
follows
57
multidimensional
big
data
can
also
be
represented
as
data
cubes
or
mathematically
tensors
array
database
systems
have
set
out
to
provide
storage
and
high-level
query
support
on
this
data
type
additional
technologies
being
applied
to
big
data
include
efficient
tensor-based
computation
58
such
as
multilinear
subspace
learning.
59
massively
parallel-processing
mpp
databases
search-based
applications
data
mining
60
distributed
file
systems
distributed
databases
cloud
and
hpc-based
infrastructure
applications
storage
and
computing
resources
61
and
the
internet
citation
needed
although
many
approaches
and
technologies
have
been
developed
it
still
remains
difficult
to
carry
out
machine
learning
with
big
data
62
some
mpp
relational
databases
have
the
ability
to
store
and
manage
petabytes
of
data
implicit
is
the
ability
to
load
monitor
back
up
and
optimize
the
use
of
the
large
data
tables
in
the
rdbms
63
promotional
source
darpa
's
topological
data
analysis
program
seeks
the
fundamental
structure
of
massive
data
sets
and
in
2008
the
technology
went
public
with
the
launch
of
company
called
ayasdi
64
third-party
source
needed
the
practitioners
of
big
data
analytics
processes
are
generally
hostile
to
slower
shared
storage
65
preferring
direct-attached
storage
das
in
its
various
forms
from
solid
state
drive
ssd
to
high
capacity
sata
disk
buried
inside
parallel
processing
nodes
the
perception
of
shared
storage
architectures—storage
area
network
san
and
network-attached
storage
nas
—is
that
they
are
relatively
slow
complex
and
expensive
these
qualities
are
not
consistent
with
big
data
analytics
systems
that
thrive
on
system
performance
commodity
infrastructure
and
low
cost
real
or
near-real
time
information
delivery
is
one
of
the
defining
characteristics
of
big
data
analytics
latency
is
therefore
avoided
whenever
and
wherever
possible
data
in
memory
is
good—data
on
spinning
disk
at
the
other
end
of
fc
san
connection
is
not
the
cost
of
san
at
the
scale
needed
for
analytics
applications
is
very
much
higher
than
other
storage
techniques
there
are
advantages
as
well
as
disadvantages
to
shared
storage
in
big
data
analytics
but
big
data
analytics
practitioners
as
of
2011
update
did
not
favour
it
66
promotional
source
big
data
increased
the
demand
of
information
management
specialists
so
much
so
that
software
ag
oracle
corporation
ibm
microsoft
sap
emc
hp
and
dell
have
spent
more
than
15
billion
on
software
firms
specializing
in
data
management
and
analytics
in
2010
this
industry
was
worth
more
than
100
billion
and
was
growing
at
almost
10
percent
year
about
twice
as
fast
as
the
software
business
as
whole
7
developed
economies
increasingly
use
data-intensive
technologies
there
are
4.6
billion
mobile-phone
subscriptions
worldwide
and
between
1
billion
and
2
billion
people
accessing
the
internet
7
between
1990
and
2005
more
than
1
billion
people
worldwide
entered
the
middle
class
which
means
more
people
became
more
literate
which
in
turn
led
to
information
growth
the
world
's
effective
capacity
to
exchange
information
through
telecommunication
networks
was
281
petabytes
in
1986
471
petabytes
in
1993
2.2
exabytes
in
2000
65
exabytes
in
2007
12
and
predictions
put
the
amount
of
internet
traffic
at
667
exabytes
annually
by
2014
7
according
to
one
estimate
one-third
of
the
globally
stored
information
is
in
the
form
of
alphanumeric
text
and
still
image
data
67
which
is
the
format
most
useful
for
most
big
data
applications
this
also
shows
the
potential
of
yet
unused
data
i.e
in
the
form
of
video
and
audio
content
while
many
vendors
offer
off-the-shelf
solutions
for
big
data
experts
recommend
the
development
of
in-house
solutions
custom-tailored
to
solve
the
company
's
problem
at
hand
if
the
company
sufficient
technical
capabilities
68
the
use
and
adoption
of
big
data
within
governmental
processes
allows
efficiencies
in
terms
of
cost
productivity
and
innovation
69
but
does
not
come
without
its
flaws
data
analysis
often
requires
multiple
parts
of
government
central
and
local
to
work
in
collaboration
and
create
new
and
innovative
processes
to
deliver
the
desired
outcome
crvs
civil
registration
and
vital
statistics
collects
all
certificates
status
from
birth
to
death
crvs
is
source
of
big
data
for
governments
research
on
the
effective
usage
of
information
and
communication
technologies
for
development
also
known
as
ict4d
suggests
that
big
data
technology
can
make
important
contributions
but
also
present
unique
challenges
to
international
development
70
71
advancements
in
big
data
analysis
offer
cost-effective
opportunities
to
improve
decision-making
in
critical
development
areas
such
as
health
care
employment
economic
productivity
crime
security
and
natural
disaster
and
resource
management
72
73
74
additionally
user-generated
data
offers
new
opportunities
to
give
the
unheard
voice
75
however
longstanding
challenges
for
developing
regions
such
as
inadequate
technological
infrastructure
and
economic
and
human
resource
scarcity
exacerbate
existing
concerns
with
big
data
such
as
privacy
imperfect
methodology
and
interoperability
issues
72
based
on
tcs
2013
global
trend
study
improvements
in
supply
planning
and
product
quality
provide
the
greatest
benefit
of
big
data
for
manufacturing
big
data
provides
an
infrastructure
for
transparency
in
manufacturing
industry
which
is
the
ability
to
unravel
uncertainties
such
as
inconsistent
component
performance
and
availability
predictive
manufacturing
as
an
applicable
approach
toward
near-zero
downtime
and
transparency
requires
vast
amount
of
data
and
advanced
prediction
tools
for
systematic
process
of
data
into
useful
information
76
conceptual
framework
of
predictive
manufacturing
begins
with
data
acquisition
where
different
type
of
sensory
data
is
available
to
acquire
such
as
acoustics
vibration
pressure
current
voltage
and
controller
data
vast
amount
of
sensory
data
in
addition
to
historical
data
construct
the
big
data
in
manufacturing
the
generated
big
data
acts
as
the
input
into
predictive
tools
and
preventive
strategies
such
as
prognostics
and
health
management
phm
77
78
big
data
analytics
helped
healthcare
improve
by
providing
personalized
medicine
and
prescriptive
analytics
clinical
risk
intervention
and
predictive
analytics
waste
and
care
variability
reduction
automated
external
and
internal
reporting
of
patient
data
standardized
medical
terms
and
patient
registries
and
fragmented
point
solutions
79
80
81
82
some
areas
of
improvement
are
more
aspirational
than
actually
implemented
the
level
of
data
generated
within
healthcare
systems
is
not
trivial
with
the
added
adoption
of
mhealth
ehealth
and
wearable
technologies
the
volume
of
data
will
continue
to
increase
this
includes
electronic
health
record
data
imaging
data
patient
generated
data
sensor
data
and
other
forms
of
difficult
to
process
data
there
is
now
an
even
greater
need
for
such
environments
to
pay
greater
attention
to
data
and
information
quality
83
``
big
data
very
often
means
'dirty
data
and
the
fraction
of
data
inaccuracies
increases
with
data
volume
growth
''
human
inspection
at
the
big
data
scale
is
impossible
and
there
is
desperate
need
in
health
service
for
intelligent
tools
for
accuracy
and
believability
control
and
handling
of
information
missed
84
while
extensive
information
in
healthcare
is
now
electronic
it
fits
under
the
big
data
umbrella
as
most
is
unstructured
and
difficult
to
use
85
the
use
of
big
data
in
healthcare
raised
significant
ethical
challenges
ranging
from
risks
for
individual
rights
privacy
and
autonomy
to
transparency
and
trust
86
mckinsey
global
institute
study
found
shortage
of
1.5
million
highly
trained
data
professionals
and
managers
57
and
number
of
universities
87
better
source
needed
including
university
of
tennessee
and
uc
berkeley
have
created
masters
programs
to
meet
this
demand
private
bootcamps
have
also
developed
programs
to
meet
that
demand
including
free
programs
like
the
data
incubator
or
paid
programs
like
general
assembly
88
in
the
specific
field
of
marketing
one
of
the
problems
stressed
by
wedel
and
kannan
89
is
that
marketing
several
subdomains
e.g.
advertising
promotions
product
development
branding
that
all
use
different
types
of
data
because
one-size-fits-all
analytical
solutions
are
not
desirable
business
schools
should
prepare
marketing
managers
to
have
wide
knowledge
on
all
the
different
techniques
used
in
these
subdomains
to
get
big
picture
and
work
effectively
with
analysts
to
understand
how
the
media
utilizes
big
data
it
is
first
necessary
to
provide
some
context
into
the
mechanism
used
for
media
process
it
been
suggested
by
nick
couldry
and
joseph
turow
that
practitioners
in
media
and
advertising
approach
big
data
as
many
actionable
points
of
information
about
millions
of
individuals
the
industry
appears
to
be
moving
away
from
the
traditional
approach
of
using
specific
media
environments
such
as
newspapers
magazines
or
television
shows
and
instead
taps
into
consumers
with
technologies
that
reach
targeted
people
at
optimal
times
in
optimal
locations
the
ultimate
aim
is
to
serve
or
convey
message
or
content
that
is
statistically
speaking
in
line
with
the
consumer
's
mindset
for
example
publishing
environments
are
increasingly
tailoring
messages
advertisements
and
content
articles
to
appeal
to
consumers
that
have
been
exclusively
gleaned
through
various
data-mining
activities
90
channel
4
the
british
public-service
television
broadcaster
is
leader
in
the
field
of
big
data
and
data
analysis
92
health
insurance
providers
are
collecting
data
on
social
``
determinants
of
health
''
such
as
food
and
tv
consumption
marital
status
clothing
size
and
purchasing
habits
from
which
they
make
predictions
on
health
costs
in
order
to
spot
health
issues
in
their
clients
it
is
controversial
whether
these
predictions
are
currently
being
used
for
pricing
93
big
data
and
the
iot
work
in
conjunction
data
extracted
from
iot
devices
provides
mapping
of
device
interconnectivity
such
mappings
have
been
used
by
the
media
industry
companies
and
governments
to
more
accurately
target
their
audience
and
increase
media
efficiency
iot
is
also
increasingly
adopted
as
means
of
gathering
sensory
data
and
this
sensory
data
been
used
in
medical
94
manufacturing
95
and
transportation
96
contexts
kevin
ashton
digital
innovation
expert
who
is
credited
with
coining
the
term
97
defines
the
internet
of
things
in
this
quote
“
if
we
had
computers
that
knew
everything
there
was
to
know
about
things—using
data
they
gathered
without
any
help
from
us—we
would
be
able
to
track
and
count
everything
and
greatly
reduce
waste
loss
and
cost
we
would
know
when
things
needed
replacing
repairing
or
recalling
and
whether
they
were
fresh
or
past
their
best.
”
especially
since
2015
big
data
come
to
prominence
within
business
operations
as
tool
to
help
employees
work
more
efficiently
and
streamline
the
collection
and
distribution
of
information
technology
it
the
use
of
big
data
to
resolve
it
and
data
collection
issues
within
an
enterprise
is
called
it
operations
analytics
itoa
98
by
applying
big
data
principles
into
the
concepts
of
machine
intelligence
and
deep
computing
it
departments
can
predict
potential
issues
and
move
to
provide
solutions
before
the
problems
even
happen
98
in
this
time
itoa
businesses
were
also
beginning
to
play
major
role
in
systems
management
by
offering
platforms
that
brought
individual
data
silos
together
and
generated
insights
from
the
whole
of
the
system
rather
than
from
isolated
pockets
of
data
examples
of
uses
of
big
data
in
public
services
big
data
can
be
used
to
improve
training
and
understanding
competitors
using
sport
sensors
it
is
also
possible
to
predict
winners
in
match
using
big
data
analytics
136
future
performance
of
players
could
be
predicted
as
well
thus
players
value
and
salary
is
determined
by
data
collected
throughout
the
season
137
in
formula
one
races
race
cars
with
hundreds
of
sensors
generate
terabytes
of
data
these
sensors
collect
data
points
from
tire
pressure
to
fuel
burn
efficiency
138
based
on
the
data
engineers
and
data
analysts
decide
whether
adjustments
should
be
made
in
order
to
win
race
besides
using
big
data
race
teams
try
to
predict
the
time
they
will
finish
the
race
beforehand
based
on
simulations
using
data
collected
over
the
season
139
encrypted
search
and
cluster
formation
in
big
data
were
demonstrated
in
march
2014
at
the
american
society
of
engineering
education
gautam
siwach
engaged
at
tackling
the
challenges
of
big
data
by
mit
computer
science
and
artificial
intelligence
laboratory
and
dr.
amir
esmailpour
at
unh
research
group
investigated
the
key
features
of
big
data
as
the
formation
of
clusters
and
their
interconnections
they
focused
on
the
security
of
big
data
and
the
orientation
of
the
term
towards
the
presence
of
different
type
of
data
in
an
encrypted
form
at
cloud
interface
by
providing
the
raw
definitions
and
real
time
examples
within
the
technology
moreover
they
proposed
an
approach
for
identifying
the
encoding
technique
to
advance
towards
an
expedited
search
over
encrypted
text
leading
to
the
security
enhancements
in
big
data
145
in
march
2012
the
white
house
announced
national
``
big
data
initiative
''
that
consisted
of
six
federal
departments
and
agencies
committing
more
than
200
million
to
big
data
research
projects
146
the
initiative
included
national
science
foundation
``
expeditions
in
computing
''
grant
of
10
million
over
5
years
to
the
amplab
147
at
the
university
of
california
berkeley
148
the
amplab
also
received
funds
from
darpa
and
over
dozen
industrial
sponsors
and
uses
big
data
to
attack
wide
range
of
problems
from
predicting
traffic
congestion
149
to
fighting
cancer
150
the
white
house
big
data
initiative
also
included
commitment
by
the
department
of
energy
to
provide
25
million
in
funding
over
5
years
to
establish
the
scalable
data
management
analysis
and
visualization
sdav
institute
151
led
by
the
energy
department
's
lawrence
berkeley
national
laboratory
the
sdav
institute
aims
to
bring
together
the
expertise
of
six
national
laboratories
and
seven
universities
to
develop
new
tools
to
help
scientists
manage
and
visualize
data
on
the
department
's
supercomputers
the
u.s.
state
of
massachusetts
announced
the
massachusetts
big
data
initiative
in
may
2012
which
provides
funding
from
the
state
government
and
private
companies
to
variety
of
research
institutions
152
the
massachusetts
institute
of
technology
hosts
the
intel
science
and
technology
center
for
big
data
in
the
mit
computer
science
and
artificial
intelligence
laboratory
combining
government
corporate
and
institutional
funding
and
research
efforts
153
the
european
commission
is
funding
the
2-year-long
big
data
public
private
forum
through
their
seventh
framework
program
to
engage
companies
academics
and
other
stakeholders
in
discussing
big
data
issues
the
project
aims
to
define
strategy
in
terms
of
research
and
innovation
to
guide
supporting
actions
from
the
european
commission
in
the
successful
implementation
of
the
big
data
economy
outcomes
of
this
project
will
be
used
as
input
for
horizon
2020
their
next
framework
program
154
the
british
government
announced
in
march
2014
the
founding
of
the
alan
turing
institute
named
after
the
computer
pioneer
and
code-breaker
which
will
focus
on
new
ways
to
collect
and
analyse
large
data
sets
155
at
the
university
of
waterloo
stratford
campus
canadian
open
data
experience
code
inspiration
day
participants
demonstrated
how
using
data
visualization
can
increase
the
understanding
and
appeal
of
big
data
sets
and
communicate
their
story
to
the
world
156
to
make
manufacturing
more
competitive
in
the
united
states
and
globe
there
is
need
to
integrate
more
american
ingenuity
and
innovation
into
manufacturing
therefore
national
science
foundation
granted
the
industry
university
cooperative
research
center
for
intelligent
maintenance
systems
ims
at
university
of
cincinnati
to
focus
on
developing
advanced
predictive
tools
and
techniques
to
be
applicable
in
big
data
environment
157
in
may
2013
ims
center
held
an
industry
advisory
board
meeting
focusing
on
big
data
where
presenters
from
various
industrial
companies
discussed
their
concerns
issues
and
future
goals
in
big
data
environment
computational
social
sciences
–
anyone
can
use
application
programming
interfaces
apis
provided
by
big
data
holders
such
as
google
and
twitter
to
do
research
in
the
social
and
behavioral
sciences
158
often
these
apis
are
provided
for
free
158
tobias
preis
et
used
google
trends
data
to
demonstrate
that
internet
users
from
countries
with
higher
per
capita
gross
domestic
product
gdp
are
more
likely
to
search
for
information
about
the
future
than
information
about
the
past
the
findings
suggest
there
may
be
link
between
online
behaviour
and
real-world
economic
indicators
159
160
161
the
authors
of
the
study
examined
google
queries
logs
made
by
ratio
of
the
volume
of
searches
for
the
coming
year
'2011
to
the
volume
of
searches
for
the
previous
year
'2009
which
they
call
the
'future
orientation
index
162
they
compared
the
future
orientation
index
to
the
per
capita
gdp
of
each
country
and
found
strong
tendency
for
countries
where
google
users
inquire
more
about
the
future
to
have
higher
gdp
the
results
hint
that
there
may
potentially
be
relationship
between
the
economic
success
of
country
and
the
information-seeking
behavior
of
its
citizens
captured
in
big
data
tobias
preis
and
his
colleagues
helen
susannah
moat
and
h.
eugene
stanley
introduced
method
to
identify
online
precursors
for
stock
market
moves
using
trading
strategies
based
on
search
volume
data
provided
by
google
trends
163
their
analysis
of
google
search
volume
for
98
terms
of
varying
financial
relevance
published
in
scientific
reports
164
suggests
that
increases
in
search
volume
for
financially
relevant
search
terms
tend
to
precede
large
losses
in
financial
markets
165
166
167
168
169
170
171
big
data
sets
come
with
algorithmic
challenges
that
previously
did
not
exist
hence
there
is
need
to
fundamentally
change
the
processing
ways
172
the
workshops
on
algorithms
for
modern
massive
data
sets
mmds
bring
together
computer
scientists
statisticians
mathematicians
and
data
analysis
practitioners
to
discuss
algorithmic
challenges
of
big
data
173
an
important
research
question
that
can
be
asked
about
big
data
sets
is
whether
you
need
to
look
at
the
full
data
to
draw
certain
conclusions
about
the
properties
of
the
data
or
is
sample
good
enough
the
name
big
data
itself
contains
term
related
to
size
and
this
is
an
important
characteristic
of
big
data
but
sampling
statistics
enables
the
selection
of
right
data
points
from
within
the
larger
data
set
to
estimate
the
characteristics
of
the
whole
population
for
example
there
are
about
600
million
tweets
produced
every
day
is
it
necessary
to
look
at
all
of
them
to
determine
the
topics
that
are
discussed
during
the
day
is
it
necessary
to
look
at
all
the
tweets
to
determine
the
sentiment
on
each
of
the
topics
in
manufacturing
different
types
of
sensory
data
such
as
acoustics
vibration
pressure
current
voltage
and
controller
data
are
available
at
short
time
intervals
to
predict
downtime
it
may
not
be
necessary
to
look
at
all
the
data
but
sample
may
be
sufficient
big
data
can
be
broken
down
by
various
data
point
categories
such
as
demographic
psychographic
behavioral
and
transactional
data
with
large
sets
of
data
points
marketers
are
able
to
create
and
utilize
more
customized
segments
of
consumers
for
more
strategic
targeting
there
been
some
work
done
in
sampling
algorithms
for
big
data
theoretical
formulation
for
sampling
twitter
data
been
developed
174
critiques
of
the
big
data
paradigm
come
in
two
flavors
those
that
question
the
implications
of
the
approach
itself
and
those
that
question
the
way
it
is
currently
done
175
one
approach
to
this
criticism
is
the
field
of
critical
data
studies
``
crucial
problem
is
that
we
do
not
know
much
about
the
underlying
empirical
micro-processes
that
lead
to
the
emergence
of
the
typical
network
characteristics
of
big
data
''
21
in
their
critique
snijders
matzat
and
reips
point
out
that
often
very
strong
assumptions
are
made
about
mathematical
properties
that
may
not
at
all
reflect
what
is
really
going
on
at
the
level
of
micro-processes
mark
graham
leveled
broad
critiques
at
chris
anderson
's
assertion
that
big
data
will
spell
the
end
of
theory
176
focusing
in
particular
on
the
notion
that
big
data
must
always
be
contextualized
in
their
social
economic
and
political
contexts
177
even
as
companies
invest
eight-
and
nine-figure
sums
to
derive
insight
from
information
streaming
in
from
suppliers
and
customers
less
than
40
of
employees
have
sufficiently
mature
processes
and
skills
to
do
so
to
overcome
this
insight
deficit
big
data
matter
how
comprehensive
or
well
analyzed
must
be
complemented
by
``
big
judgment
''
according
to
an
article
in
the
harvard
business
review
178
much
in
the
same
line
it
been
pointed
out
that
the
decisions
based
on
the
analysis
of
big
data
are
inevitably
``
informed
by
the
world
as
it
was
in
the
past
or
at
best
as
it
currently
is
''
72
fed
by
large
number
of
data
on
past
experiences
algorithms
can
predict
future
development
if
the
future
is
similar
to
the
past
179
if
the
systems
dynamics
of
the
future
change
if
it
is
not
stationary
process
the
past
can
say
little
about
the
future
in
order
to
make
predictions
in
changing
environments
it
would
be
necessary
to
have
thorough
understanding
of
the
systems
dynamic
which
requires
theory
179
as
response
to
this
critique
alemany
oliver
and
vayre
suggest
to
use
``
abductive
reasoning
as
first
step
in
the
research
process
in
order
to
bring
context
to
consumers
digital
traces
and
make
new
theories
emerge
''
180
additionally
it
been
suggested
to
combine
big
data
approaches
with
computer
simulations
such
as
agent-based
models
72
and
complex
systems
agent-based
models
are
increasingly
getting
better
in
predicting
the
outcome
of
social
complexities
of
even
unknown
future
scenarios
through
computer
simulations
that
are
based
on
collection
of
mutually
interdependent
algorithms
181
182
finally
use
of
multivariate
methods
that
probe
for
the
latent
structure
of
the
data
such
as
factor
analysis
and
cluster
analysis
have
proven
useful
as
analytic
approaches
that
go
well
beyond
the
bi-variate
approaches
cross-tabs
typically
employed
with
smaller
data
sets
in
health
and
biology
conventional
scientific
approaches
are
based
on
experimentation
for
these
approaches
the
limiting
factor
is
the
relevant
data
that
can
confirm
or
refute
the
initial
hypothesis
183
new
postulate
is
accepted
now
in
biosciences
the
information
provided
by
the
data
in
huge
volumes
omics
without
prior
hypothesis
is
complementary
and
sometimes
necessary
to
conventional
approaches
based
on
experimentation
184
185
in
the
massive
approaches
it
is
the
formulation
of
relevant
hypothesis
to
explain
the
data
that
is
the
limiting
factor
186
the
search
logic
is
reversed
and
the
limits
of
induction
``
glory
of
science
and
philosophy
scandal
''
c.
d.
broad
1926
are
to
be
considered
citation
needed
privacy
advocates
are
concerned
about
the
threat
to
privacy
represented
by
increasing
storage
and
integration
of
personally
identifiable
information
expert
panels
have
released
various
policy
recommendations
to
conform
practice
to
expectations
of
privacy
187
188
189
the
misuse
of
big
data
in
several
cases
by
media
companies
and
even
the
government
allowed
for
abolition
of
trust
in
almost
every
fundamental
institution
holding
up
society
190
nayef
al-rodhan
argues
that
new
kind
of
social
contract
will
be
needed
to
protect
individual
liberties
in
context
of
big
data
and
giant
corporations
that
own
vast
amounts
of
information
the
use
of
big
data
should
be
monitored
and
better
regulated
at
the
national
and
international
levels
191
barocas
and
nissenbaum
argue
that
one
way
of
protecting
individual
users
is
by
being
informed
about
the
types
of
information
being
collected
with
whom
it
is
shared
under
what
constrains
and
for
what
purposes
192
the
v
model
of
big
data
is
concerting
as
it
centres
around
computational
scalability
and
lacks
in
loss
around
the
perceptibility
and
understandability
of
information
this
led
to
the
framework
of
cognitive
big
data
which
characterises
big
data
application
according
to
193
large
data
sets
have
been
analyzed
by
computing
machines
for
well
over
century
including
the
1890s
us
census
analytics
performed
by
ibm
's
punch
card
machines
which
computed
statistics
including
means
and
variances
of
populations
across
the
whole
continent
in
more
recent
decades
science
experiments
such
as
cern
have
produced
data
on
similar
scales
to
current
commercial
``
big
data
''
however
science
experiments
have
tended
to
analyze
their
data
using
specialized
custom-built
high
performance
computing
supercomputing
clusters
and
grids
rather
than
clouds
of
cheap
commodity
computers
as
in
the
current
commercial
wave
implying
difference
in
both
culture
and
technology
stack
ulf-dietrich
reips
and
uwe
matzat
wrote
in
2014
that
big
data
had
become
``
fad
''
in
scientific
research
158
researcher
danah
boyd
raised
concerns
about
the
use
of
big
data
in
science
neglecting
principles
such
as
choosing
representative
sample
by
being
too
concerned
about
handling
the
huge
amounts
of
data
194
this
approach
may
lead
to
results
bias
in
one
way
or
another
integration
across
heterogeneous
data
resources—some
that
might
be
considered
big
data
and
others
not—presents
formidable
logistical
as
well
as
analytical
challenges
but
many
researchers
argue
that
such
integrations
are
likely
to
represent
the
most
promising
new
frontiers
in
science
195
in
the
provocative
article
``
critical
questions
for
big
data
''
196
the
authors
title
big
data
part
of
mythology
``
large
data
sets
offer
higher
form
of
intelligence
and
knowledge
...
with
the
aura
of
truth
objectivity
and
accuracy
''
users
of
big
data
are
often
``
lost
in
the
sheer
volume
of
numbers
''
and
``
working
with
big
data
is
still
subjective
and
what
it
quantifies
does
not
necessarily
have
closer
claim
on
objective
truth
''
196
recent
developments
in
bi
domain
such
as
pro-active
reporting
especially
target
improvements
in
usability
of
big
data
through
automated
filtering
of
non-useful
data
and
correlations
197
big
structures
are
full
of
spurious
correlations
198
either
because
of
non-causal
coincidences
law
of
truly
large
numbers
solely
nature
of
big
randomness
199
ramsey
theory
or
existence
of
non
included
factors
so
the
hope
of
early
experimenters
to
make
large
databases
of
numbers
``
speak
for
themselves
''
and
revolutionize
scientific
method
is
questioned
200
big
data
analysis
is
often
shallow
compared
to
analysis
of
smaller
data
sets
201
in
many
big
data
projects
there
is
large
data
analysis
happening
but
the
challenge
is
the
extract
transform
load
part
of
data
preprocessing
201
big
data
is
buzzword
and
``
vague
term
''
202
203
but
at
the
same
time
an
``
obsession
''
203
with
entrepreneurs
consultants
scientists
and
the
media
big
data
showcases
such
as
google
flu
trends
failed
to
deliver
good
predictions
in
recent
years
overstating
the
flu
outbreaks
by
factor
of
two
similarly
academy
awards
and
election
predictions
solely
based
on
twitter
were
more
often
off
than
on
target
big
data
often
poses
the
same
challenges
as
small
data
adding
more
data
does
not
solve
problems
of
bias
but
may
emphasize
other
problems
in
particular
data
sources
such
as
twitter
are
not
representative
of
the
overall
population
and
results
drawn
from
such
sources
may
then
lead
to
wrong
conclusions
google
translate—which
is
based
on
big
data
statistical
analysis
of
text—does
good
job
at
translating
web
pages
however
results
from
specialized
domains
may
be
dramatically
skewed
on
the
other
hand
big
data
may
also
introduce
new
problems
such
as
the
multiple
comparisons
problem
simultaneously
testing
large
set
of
hypotheses
is
likely
to
produce
many
false
results
that
mistakenly
appear
significant
ioannidis
argued
that
``
most
published
research
findings
are
false
''
204
due
to
essentially
the
same
effect
when
many
scientific
teams
and
researchers
each
perform
many
experiments
i.e
process
big
amount
of
scientific
data
although
not
with
big
data
technology
the
likelihood
of
``
significant
''
result
being
false
grows
fast
–
even
more
so
when
only
positive
results
are
published
furthermore
big
data
analytics
results
are
only
as
good
as
the
model
on
which
they
are
predicated
in
an
example
big
data
took
part
in
attempting
to
predict
the
results
of
the
2016
u.s.
presidential
election
205
with
varying
degrees
of
success
macrodatos
1
​
llamados
datos
masivos
inteligencia
datos
datos
gran
escala
big
data
terminología
idioma
inglés
utilizada
comúnmente
término
hace
referencia
concepto
relativo
conjuntos
datos
tan
grandes
complejos
hagan
falta
aplicaciones
informáticas
tradicionales
procesamiento
datos
tratarlos
adecuadamente
ende
procedimientos
usados
encontrar
patrones
repetitivos
dentro
datos
sofisticados
requieren
software
especializado
textos
científicos
español
frecuencia
usa
directamente
término
inglés
big
data
tal
aparece
ensayo
viktor
schönberger
revolución
datos
masivos
2
​
3
​
uso
moderno
término
``
big
data
''
tiende
referirse
análisis
comportamiento
usuario
extrayendo
valor
datos
almacenados
formulando
predicciones
través
patrones
observados
disciplina
dedicada
datos
masivos
enmarca
sector
tecnologías
información
comunicación
disciplina
ocupa
todas
actividades
relacionadas
sistemas
manipulan
grandes
conjuntos
datos
dificultades
habituales
vinculadas
gestión
cantidades
datos
centran
recolección
almacenamiento
4
​
búsqueda
compartición
análisis
5
​
visualización
tendencia
manipular
enormes
cantidades
datos
debe
necesidad
casos
incluir
dicha
información
creación
informes
estadísticos
modelos
predictivos
utilizados
diversas
materias
análisis
negocio
publicitarios
datos
enfermedades
infecciosas
espionaje
seguimiento
población
lucha
crimen
organizado
6
​
límite
superior
procesamiento
ido
creciendo
largo
años
7
​
estima
mundo
almacenó
5
zettabytes
2014.
si
pone
información
libros
convirtiendo
imágenes
equivalente
letras
podría
hacer
4500
pilas
libros
lleguen
sol
8
​
científicos
cierta
regularidad
encuentran
límites
análisis
debido
gran
cantidad
datos
ciertas
áreas
tales
meteorología
genómica
9
​
conectómica
complejas
simulaciones
procesos
físicos
10
​
investigaciones
relacionadas
procesos
biológicos
ambientales
11
​
limitaciones
afectan
motores
búsqueda
internet
sistemas
finanzas
informática
negocios
data
sets
crecen
volumen
debido
parte
recolección
masiva
información
procedente
sensores
inalámbricos
dispositivos
móviles
ejemplo
vanet
constante
crecimiento
históricos
aplicaciones
ejemplo
registros
cámaras
sistemas
teledetección
micrófonos
lectores
identificación
radiofrecuencia
12
​
13
​
capacidad
tecnológica
per
cápita
nivel
mundial
almacenar
datos
dobla
aproximadamente
cada
cuarenta
meses
años
1980
14
​
estima
2012
cada
día
creados
cerca
2.5
trillones
bytes
datos
15
​
sistemas
gestión
bases
datos
relacionales
paquetes
software
utilizados
visualizar
datos
menudo
dificultades
manejar
big
data
trabajo
puede
requerir
``
software
masivamente
paralelo
ejecute
decenas
cientos
incluso
miles
servidores
''
16
​
califica
``
big
data
''
varía
según
capacidades
usuarios
herramientas
capacidades
expansión
hacen
big
data
objetivo
movimiento
``
organizaciones
enfrentar
cientos
gigabytes
datos
primera
vez
puede
provocar
necesidad
reconsiderar
opciones
administración
datos
puede
tomar
decenas
cientos
terabytes
tamaño
datos
convierta
consideración
importante
''
17
​
término
uso
década
1990
otorgan
crédito
john
mashey
18
​
popularizarlo
big
data
macrodatos
término
hace
referencia
cantidad
datos
tal
supera
capacidad
software
convencional
ser
capturados
administrados
procesados
tiempo
razonable
volumen
datos
masivos
crece
constantemente
2012
estimaba
tamaño
docena
terabytes
varios
petabyte
datos
único
conjunto
datos
metodología
mike2.0
dedicada
investigar
temas
relacionados
gestión
información
definen
big
data
19
​
términos
permutaciones
útiles
complejidad
dificultad
borrar
registros
individuales
definido
datos
suficientemente
masivos
poner
relieve
cuestiones
preocupaciones
torno
efectividad
anonimato
perspectiva
práctica
teórica
20
​
2001
informe
investigación
fundamentaba
congresos
presentaciones
relacionadas
21
​
meta
group
ahora
gartner
definía
crecimiento
constante
datos
oportunidad
reto
investigar
volumen
velocidad
variedad
gartner
continúa
usando
datos
masivos
referencia
22
​
además
grandes
proveedores
mercado
datos
masivos
desarrollando
soluciones
atender
demandas
críticas
cómo
procesar
tal
cantidad
datos
mapr
cloudera
definición
2016
establece
``
big
data
representa
activos
información
caracterizados
volumen
velocidad
variedad
tan
altos
requieren
tecnología
específica
métodos
analíticos
transformación
valor
''
23
​
además
organizaciones
agregan
nueva
v
veracidad
describirlo
24
​
revisionismo
cuestionado
autoridades
industria
25
​
tres
v
volumen
variedad
velocidad
ampliado
características
complementarias
big
data
definición
2018
establece
``
big
data
necesitan
herramientas
informáticas
paralelas
manejar
datos
''
señala
``
representa
cambio
distinto
claramente
definido
informática
utilizada
través
teorías
programación
paralelas
pérdidas
garantías
capacidades
hechas
modelo
relacional
codd
``
27
​
creciente
madurez
concepto
describe
manera
nítida
diferencia
``
big
data
''
``
business
intelligence
''
macrodatos
pueden
describir
siguientes
características
30
​
repositorios
big
data
existido
muchas
formas
menudo
creadas
corporaciones
necesidad
especial
históricamente
proveedores
comerciales
ofrecían
sistemas
administración
bases
datos
paralelos
big
data
partir
década
1990.
años
wintercorp
publicó
informe
base
datos
grande
32
​
teradata
corporation
1984
comercializó
sistema
procesamiento
paralelo
dbc
1012.
sistemas
teradata
primeros
almacenar
analizar
1
terabyte
datos
1992.
discos
duros
"2,5"
gb
1991
definición
big
data
evoluciona
continuamente
según
ley
kryder
teradata
instaló
primer
sistema
basado
rdbms
clase
petabyte
2007.
partir
2017
unas
pocas
docenas
bases
datos
relacionales
teradata
clase
petabyte
instaladas
mayor
cuales
excede
50
pb
sistemas
2008
datos
relacionales
estructurados
100
entonces
teradata
agregado
tipos
datos
estructurados
incluidos
xml
json
avro
2000
seisint
inc.
ahora
lexisnexis
group
desarrolló
marco
intercambio
archivos
distribuido
basado
c++
almacenamiento
consultas
datos
sistema
almacena
distribuye
datos
estructurados
semiestructurados
estructurados
varios
servidores
usuarios
pueden
crear
consultas
dialecto
c++
llamado
ecl
ecl
utiliza
método
``
aplicar
esquema
lectura
''
inferir
estructura
datos
almacenados
consulta
lugar
almacena
2004
lexisnexis
adquirió
seisint
inc.
33
​
2008
adquirió
choicepoint
inc.
34
​y
plataforma
procesamiento
paralelo
alta
velocidad
dos
plataformas
fusionaron
sistemas
hpcc
cluster
computación
alto
rendimiento
2011
hpcc
código
abierto
bajo
licencia
apache
v2.0
quantcast
file
system
disponible
aproximadamente
mismo
tiempo
35
​
cern
experimentos
física
recopilado
grandes
conjuntos
datos
muchas
décadas
generalmente
analizados
través
computadoras
alto
rendimiento
supercomputadores
lugar
arquitecturas
mapas
reducidos
productos
generalmente
refieren
movimiento
actual
``
big
data
''
2004
google
publicó
documento
proceso
llamado
mapreduce
utiliza
arquitectura
similar
concepto
mapreduce
proporciona
modelo
procesamiento
paralelo
lanzó
implementación
asociada
procesar
grandes
cantidades
datos
mapreduce
consultas
dividen
distribuyen
través
nodos
paralelos
procesan
paralelo
paso
mapa
resultados
recopilan
entregan
paso
reducir
marco
exitoso
quisieron
replicar
algoritmo
implementación
marco
mapreduce
adoptada
proyecto
código
abierto
apache
llamado
hadoop
36
​apache
spark
desarrolló
2012
respuesta
limitaciones
paradigma
mapreduce
agrega
capacidad
configurar
muchas
operaciones
solo
mapa
seguido
reducción
mike2.0
enfoque
abierto
administración
información
reconoce
necesidad
revisiones
debido
implicaciones
big
data
identificadas
artículo
titulado
``
oferta
soluciones
big
data
''
37
​la
metodología
aborda
manejo
big
data
términos
permutaciones
útiles
fuentes
datos
complejidad
interrelaciones
dificultad
eliminar
modificar
registros
individuales
38
​
estudios
2012
mostraron
arquitectura
capas
múltiples
opción
abordar
problemas
presenta
big
data
arquitectura
paralela
distribuida
distribuye
datos
múltiples
servidores
entornos
ejecución
paralela
pueden
mejorar
drásticamente
velocidades
procesamiento
datos
tipo
arquitectura
inserta
datos
dbms
paralelo
implementa
uso
marcos
mapreduce
hadoop
tipo
marco
busca
hacer
poder
procesamiento
transparente
usuario
final
mediante
uso
servidor
aplicaciones
usuario
39
​
análisis
big
data
aplicaciones
fabricación
comercializa
arquitectura
5c
conexión
conversión
cibernética
cognición
configuración
40
​
lago
datos
permite
organización
cambie
enfoque
control
centralizado
modelo
compartido
responder
dinámica
cambiante
administración
información
permite
segregación
rápida
datos
lago
datos
reduce
tiempo
sobrecarga
41
​
42
​
existen
muchísimas
herramientas
manejo
big
data
ejemplos
incluyen
hadoop
nosql
cassandra
inteligencia
empresarial
aprendizaje
automático
mapreduce
herramientas
tratan
tres
tipos
big
data
43
​
informe
2011
mckinsey
global
institute
caracteriza
componentes
principales
ecosistema
big
data
siguiente
manera
45
​
big
data
multidimensionales
pueden
representar
cubos
datos
matemáticamente
tensores
sistemas
bases
datos
array
propuesto
proporcionar
almacenamiento
soporte
consultas
alto
nivel
tipo
datos
tecnologías
adicionales
aplican
big
data
incluyen
cálculo
basado
tensor
eficiente
46
​
aprendizaje
subespacio
multilineal
47
​
bases
datos
procesamiento
paralelo
masivo
mpp
aplicaciones
basadas
búsqueda
extracción
datos
48
​
sistemas
archivos
distribuidos
bases
datos
distribuidas
nube
infraestructura
basada
hpc
aplicaciones
almacenamiento
recursos
informáticos
49
​
internet
pesar
desarrollado
enfoques
tecnologías
sigue
siendo
difícil
llevar
cabo
aprendizaje
automático
grandes
datos
50
​
bases
datos
relacionales
mpp
capacidad
almacenar
administrar
petabytes
datos
implícita
capacidad
cargar
supervisar
realizar
copias
seguridad
optimizar
uso
tablas
datos
gran
tamaño
rdbms
51
​
programa
análisis
topológico
datos
darpa
busca
estructura
fundamental
conjuntos
datos
masivos
2008
tecnología
hizo
pública
lanzamiento
compañía
llamada
ayasdi
52
​
profesionales
procesos
análisis
big
data
generalmente
hostiles
almacenamiento
compartido
lento
53
​
prefieren
almacenamiento
conexión
directa
das
diversas
formas
unidad
sólido
ssd
disco
sata
gran
capacidad
enterrado
dentro
nodos
procesamiento
paralelo
percepción
arquitecturas
almacenamiento
compartidas
red
área
almacenamiento
san
almacenamiento
conectado
red
nas
relativamente
lentas
complejas
costosas
cualidades
consistentes
sistemas
análisis
datos
grandes
prosperan
rendimiento
sistema
infraestructura
productos
básicos
bajo
costo
entrega
información
real
casi
tiempo
real
características
definitorias
análisis
big
data
evita
latencia
siempre
posible
datos
memoria
buenos
datos
disco
giratorio
extremo
conexión
fc
san
costo
san
escala
necesaria
aplicaciones
analíticas
mayor
técnicas
almacenamiento
ventajas
desventajas
almacenamiento
compartido
análisis
big
data
practicantes
análisis
big
data
partir
2011
favorecieron
¿de
dónde
provienen
datos
fabricamos
directa
indirectamente
segundo
tras
segundo
iphone
hoy
día
capacidad
cómputo
nasa
ser
humano
llegó
luna
54
​
cantidad
datos
generados
persona
unidad
tiempo
grande
catalogamos
procedencia
datos
según
siguientes
categorías
55
​
vez
encontradas
fuentes
datos
necesarios
posiblemente
dispongamos
sinfín
tablas
origen
relacionadas
siguiente
objetivo
hacer
datos
recojan
mismo
lugar
darles
formato
adecuado
aquí
entran
juego
plataformas
extraer
transformar
cargar
etl
propósito
extraer
datos
diferentes
fuentes
sistemas
después
hacer
transformaciones
conversiones
datos
limpieza
datos
sucios
cambios
formato
etc
finalmente
cargar
datos
base
datos
almacén
datos
especificada
60
​
ejemplo
plataforma
etl
pentaho
data
integration
concretamente
aplicación
spoon
término
nosql
refiere
not
only
sql
solo
sql
sistemas
almacenamiento
cumplen
esquema
entidad-relación
61
​
proveen
sistema
almacenamiento
flexible
concurrente
permiten
manipular
grandes
cantidades
información
manera
rápida
bases
datos
relacionales
distinguimos
cuatro
grandes
grupos
bases
datos
nosql
útiles
operaciones
simples
basadas
claves
ejemplo
aumento
velocidad
carga
sitio
web
puede
utilizar
diferentes
perfiles
usuario
mapeados
archivos
incluir
según
id
usuario
sido
calculados
anterioridad
apache
cassandra
tecnología
almacenamiento
clave-valor
reconocida
usuarios
64
​
couchdb
mongodb
64
​
quizá
conocidas
hacer
mención
especial
mapreduce
tecnología
google
inicialmente
diseñada
algoritmo
pagerank
permite
seleccionar
subconjunto
datos
agruparlos
reducirlos
cargarlos
colección
hadoop
tecnología
apache
diseñada
almacenar
procesar
grandes
cantidades
datos
análisis
permite
mirar
datos
explicar
pasando
datos
necesarios
almacenados
según
diferentes
tecnologías
almacenamiento
daremos
cuenta
necesitaremos
diferentes
técnicas
análisis
datos
siguientes
tal
instituto
nacional
estadística
dice
tutoriales
«
imagen
vale
mil
palabras
mil
datos
»
mente
agradece
presentación
bien
estructurada
resultados
estadísticos
gráficos
mapas
vez
tablas
números
conclusiones
macrodatos
llega
paso
allá
parafraseando
edward
tufte
expertos
visualización
datos
reconocidos
nivel
mundial
«
mundo
complejo
dinámico
multidimensional
papel
estático
plano
¿cómo
vamos
representar
rica
experiencia
visual
mundo
mera
planicie
»
mondrian
69
​
plataforma
permite
visualizar
información
través
análisis
llevados
cabo
datos
disponemos
plataforma
intenta
llegar
público
concreto
utilidad
acotada
cuadro
mando
integral
organización
últimos
años
generalizado
plataformas
tableau
power
bi
qlik
70
​
lado
infografías
vuelto
fenómeno
viral
recogen
resultados
diferentes
análisis
datos
material
atractivo
entretenido
simplificado
audiencias
masivas
71
​
uso
big
data
sido
utilizado
industria
medios
empresas
gobiernos
dirigirse
mayor
precisión
público
aumentar
eficiencia
mensajes
big
data
aumentado
demanda
especialistas
administración
información
software
ag
oracle
corporation
ibm
microsoft
sap
emc
hp
dell
gastado
15
mil
millones
firmas
software
especializadas
administración
análisis
datos
2010
industria
valía
100
mil
millones
crecía
casi
10
ciento
anual
aproximadamente
doble
rápido
negocio
software
general
72
​
economías
desarrolladas
usan
cada
vez
tecnologías
intensivas
datos
4.600
millones
suscripciones
teléfonos
móviles
mundo
1.000
2.000
millones
personas
acceden
internet
1990
2005
mil
millones
personas
mundo
ingresaron
clase
media
significa
personas
volvieron
alfabetizadas
vez
llevó
crecimiento
información
capacidad
efectiva
mundial
intercambiar
información
través
redes
telecomunicaciones
281
petabytes
1986
471
petabytes
1993
2.2
exabytes
2000
65
exabytes
2007
73
​
predicciones
cifran
tráfico
internet
667
exabytes
anualmente
2014.
según
estimación
tercio
información
almacenada
mundo
forma
texto
alfanumérico
imágenes
fijas
74
​
formato
útil
mayoría
aplicaciones
big
data
muestra
potencial
datos
aún
utilizados
decir
forma
contenido
video
audio
si
bien
proveedores
ofrecen
soluciones
estándar
big
data
expertos
recomiendan
desarrollo
soluciones
internas
personalizadas
resolver
problema
compañía
si
empresa
cuenta
capacidades
técnicas
suficientes
75
​
uso
adopción
big
data
dentro
procesos
gubernamentales
permite
eficiencias
términos
costo
productividad
innovación
viene
defectos
76
​
análisis
datos
menudo
requiere
varias
partes
gobierno
central
local
trabajen
colaboración
creen
procesos
nuevos
lograr
resultado
deseado
datos
masivos
usan
habitualmente
influenciar
proceso
democrático
representantes
pueblo
pueden
ver
hacen
ciudadanos
ciudadanos
pueden
dictar
vida
pública
representantes
mediante
tuits
métodos
extender
ideas
sociedad
campañas
presidenciales
obama
trump
usaron
manera
generalizada
77
​
expertos
advierten
«
reinventar
democracia
representativa
si
posible
convierta
dictadura
información
»
78
​
banco
interamericano
desarrollo
bid
desarrollado
estudios
américa
latina
presenta
distintos
casos
uso
macrodatos
diseño
implementación
políticas
públicas
destacando
intervenciones
temas
movilidad
urbana
ciudades
intelgientes
seguridad
temáticas
recomendacione
mismos
girado
torno
cómo
construir
instituciones
públicas
logren
mediante
uso
datos
masivos
ser
transparentes
ayuden
tomar
mejores
decisiones
79
​
investigación
uso
efectivo
tecnologías
información
comunicación
desarrollo
conocido
ict4d
sugiere
tecnología
big
data
puede
hacer
contribuciones
importantes
presentar
desafíos
únicos
desarrollo
internacional
80
​
81
​
avances
análisis
big
data
ofrecen
oportunidades
rentables
mejorar
toma
decisiones
áreas
desarrollo
críticas
atención
médica
empleo
productividad
económica
delincuencia
seguridad
manejo
recursos
desastres
naturales
82
​
además
datos
generados
usuario
ofrecen
nuevas
oportunidades
ofrecer
voz
inaudita
embargo
desafíos
larga
data
regiones
desarrollo
infraestructura
tecnológica
inadecuada
escasez
recursos
económicos
humanos
exacerban
preocupaciones
existentes
grandes
datos
privacidad
metodología
imperfecta
problemas
interoperabilidad
82
​
big
data
proporciona
infraestructura
transparencia
industria
manufacturera
capacidad
desentrañar
incertidumbres
rendimiento
disponibilidad
componentes
inconsistentes
fabricación
predictiva
enfoque
aplicable
tiempo
inactividad
transparencia
cercanos
cero
requiere
gran
cantidad
datos
herramientas
predicción
avanzadas
proceso
sistemático
datos
información
útil
83
​
marco
conceptual
fabricación
predictiva
comienza
adquisición
datos
encuentran
disponibles
diferentes
tipos
datos
sensoriales
tales
acústica
vibración
presión
corriente
voltaje
datos
controlador
gran
cantidad
datos
sensoriales
además
datos
históricos
construyen
grandes
datos
fabricación
big
data
generados
actúan
entrada
herramientas
predictivas
estrategias
preventivas
pronósticos
gestión
salud
phm
84
​
profesionales
medios
publicidad
abordan
grandes
datos
puntos
información
procesables
millones
personas
industria
parece
alejarse
enfoque
tradicional
utilizar
entornos
medios
específicos
periódicos
revistas
programas
televisión
lugar
aprovecha
consumidores
tecnologías
llegan
personas
objetivo
momentos
óptimos
ubicaciones
óptimas
objetivo
final
servir
transmitir
mensaje
contenido
estadísticamente
hablando
línea
mentalidad
consumidor
ejemplo
entornos
publicación
adaptan
cada
vez
mensajes
anuncios
publicitarios
contenido
artículos
atraer
consumidores
sido
recolectados
exclusivamente
través
diversas
actividades
extracción
datos
85
​
proveedores
seguro
médico
recopilan
datos
``
determinantes
sociales
''
consumo
alimentos
televisión
civil
tamaño
vestimenta
hábitos
compra
cuales
hacen
predicciones
costos
salud
detectar
problemas
salud
clientes
controvertido
si
predicciones
utilizando
actualmente
fijar
precios
87
​
ámbito
mueve
dinero
suelen
utilizar
nuevas
tecnologías
usuarios
base
encontramos
ejemplo
análisis
partidos
constituye
parte
fundamental
entrenamiento
profesionales
toma
decisiones
entrenadores
amisco
88
​
sistema
aplicado
importantes
equipos
ligas
española
francesa
alemana
inglesa
2001.
consta
8
cámaras
diversos
ordenadores
instalados
estadios
registran
movimientos
jugadores
razón
25
registros
segundo
luego
envían
datos
central
hacen
análisis
masivo
datos
información
devuelve
resultado
incluye
reproducción
partido
dos
dimensiones
datos
técnicos
estadísticas
resumen
datos
físicos
cada
jugador
permitiendo
seleccionar
varias
dimensiones
visualizaciones
diferentes
datos
88
​
crecimiento
datos
mundo
financiero
obliga
uso
big
data
procesamiento
rápido
datos
gestión
omnicanalidad
segmentación
avanzada
clientes
creación
estrategias
precios
dinámicos
gestión
riesgos
prevención
fraudes
apoyo
toma
decisiones
detectar
tendencias
consumo
definir
nuevas
formas
hacer
mejor
cosas
detectar
alertas
tipo
eventos
complejos
hacer
seguimiento
avanzado
competencia
89
​
big
data
cada
vez
utiliza
segmentación
avanzada
consumidores
automatizar
personalización
productos
adaptar
comunicaciones
momento
ciclo
venta
captar
nuevas
oportunidades
venta
apoyo
toma
decisiones
tiempo
real
gestión
crisis
90
​
91
​
búsqueda
encriptada
formación
clúster
big
data
demostraron
marzo
2014
sociedad
estadounidense
educación
ingeniería
gautam
siwach
participó
abordar
desafíos
big
data
laboratorio
ciencias
computación
inteligencia
artificial
mit
amir
esmailpour
grupo
investigación
unh
investigó
características
clave
big
data
formación
clusters
interconexiones
centraron
seguridad
macrodatos
orientación
término
hacia
presencia
diferentes
tipos
datos
forma
cifrada
interfaz
nube
proporcionar
definiciones
procesar
ejemplos
tiempo
real
dentro
tecnología
además
propusieron
enfoque
identificar
técnica
codificación
avanzar
hacia
búsqueda
acelerada
texto
encriptado
conduzca
mejoras
seguridad
big
data
92
​
marzo
2012
casa
blanca
anunció
``
iniciativa
big
data
''
nacional
consistía
seis
departamentos
agencias
federales
comprometiendo
200
millones
proyectos
investigación
big
data
iniciativa
incluyó
subvención
national
science
foundation
``
expeditions
in
computing
''
10
millones
5
años
amplab
93
​
universidad
california
berkeley
94
​
amplab
recibió
fondos
darpa
docena
patrocinadores
industriales
utiliza
big
data
atacar
amplia
gama
problemas
predecir
congestión
tráfico
95
​
combatir
cáncer
96
​
iniciativa
big
data
casa
blanca
incluyó
compromiso
departamento
energía
proporcionar
25
millones
financiamiento
5
años
establecer
instituto
administración
análisis
visualización
datos
escalables
sdav
97
​
dirigido
lawrence
berkeley
national
laboratory
departamento
energía
laboratorio
instituto
sdav
objetivo
reunir
experiencia
seis
laboratorios
nacionales
siete
universidades
desarrollar
nuevas
herramientas
ayuden
científicos
gestionar
visualizar
datos
supercomputadoras
departamento
massachusetts
anunció
iniciativa
big
data
massachusetts
mayo
2012
proporciona
fondos
gobierno
estatal
empresas
privadas
variedad
instituciones
investigación
instituto
tecnología
massachusetts
alberga
centro
ciencia
tecnología
intel
big
data
laboratorio
ciencias
computación
inteligencia
artificial
mit
combina
fondos
esfuerzos
investigación
gubernamentales
corporativos
institucionales
98
​
comisión
europea
financiando
foro
público
privado
big
data
duró
dos
años
través
séptimo
programa
framework
involucrar
empresas
académicos
partes
interesadas
discusión
problemas
big
data
proyecto
objetivo
definir
estrategia
términos
investigación
innovación
guiar
acciones
apoyo
comisión
europea
implementación
exitosa
economía
big
data
resultados
proyecto
utilizarán
aportación
horizonte
2020
próximo
programa
gobierno
británico
anunció
marzo
2014
fundación
instituto
alan
turing
lleva
nombre
pionero
informática
descifrador
códigos
centrará
nuevas
formas
recopilar
analizar
grandes
conjuntos
datos
99
​
día
inspiración
canadian
open
data
experience
code
universidad
waterloo
stratford
campus
participantes
demostraron
cómo
uso
visualización
datos
puede
aumentar
comprensión
atractivo
grandes
conjuntos
datos
comunicar
historia
mundo
100
​
fabricación
competitiva
unidos
mundo
necesario
integrar
ingenio
innovación
estadounidenses
fabricación
national
science
foundation
otorgado
centro
investigación
cooperativa
industry
industry
intelligent
maintenance
systems
ims
universidad
cincinnati
concentre
desarrollo
herramientas
técnicas
predictivas
avanzadas
aplicables
entorno
big
data
101
​
mayo
2013
ims
center
celebró
reunión
junta
asesora
industria
centrada
big
data
presentadores
varias
compañías
industriales
discutieron
preocupaciones
problemas
objetivos
futuros
entorno
big
data
ciencias
sociales
computacionales
cualquier
persona
puede
usar
interfaces
programación
aplicaciones
api
proporcionadas
grandes
titulares
datos
google
twitter
realizar
investigaciones
ciencias
sociales
comportamiento
102
​
menudo
api
proporcionan
forma
gratuita
tobias
preis
usó
datos
tendencias
google
demostrar
usuarios
internet
países
producto
interno
bruto
pib
per
cápita
alto
probabilidades
buscar
información
futuro
información
pasado
hallazgos
sugieren
puede
haber
vínculo
comportamiento
línea
indicadores
económicos
mundo
real
103
​
104
​
105
​
autores
estudio
examinaron
registros
consultas
google
realizados
relación
volumen
búsquedas
año
siguiente
'2011
volumen
búsquedas
año
anterior
'2009
denominaron
'índice
orientación
futura
106
​
compararon
índice
orientación
futura
pib
per
cápita
cada
país
encontraron
fuerte
tendencia
países
usuarios
google
informan
futuro
tener
pib
alto
resultados
sugieren
potencialmente
puede
haber
relación
éxito
económico
país
comportamiento
búsqueda
información
ciudadanos
capturado
big
data
tobias
preis
colegas
helen
susannah
moat
h.
eugene
stanley
introdujeron
método
identificar
precursores
línea
movimientos
bursátiles
utilizando
estrategias
negociación
basadas
datos
volumen
búsquedas
provistos
google
trends
107
​
análisis
volumen
búsqueda
google
98
términos
relevancia
financiera
variable
publicado
scientific
reports
108
​
sugiere
aumentos
volumen
búsqueda
términos
búsqueda
relevantes
financieramente
tienden
preceder
grandes
pérdidas
mercados
financieros
109
​
110
​
111
​
112
​
113
​
114
​
grandes
conjuntos
datos
vienen
desafíos
algorítmicos
anteriormente
existían
existe
necesidad
cambiar
fundamentalmente
formas
procesamiento
talleres
algoritmos
conjuntos
datos
masivos
modernos
mmds
reúnen
científicos
informáticos
estadísticos
matemáticos
profesionales
análisis
datos
analizar
desafíos
algorítmicos
big
data
115
​
pregunta
investigación
importante
puede
hacer
conjuntos
datos
grandes
si
necesita
ver
datos
completos
sacar
ciertas
conclusiones
propiedades
datos
si
muestra
suficientemente
buena
nombre
big
data
contiene
término
relacionado
tamaño
característica
importante
big
data
muestreo
estadísticas
permite
selección
puntos
datos
correctos
dentro
conjunto
datos
grande
estimar
características
toda
población
ejemplo
alrededor
600
millones
tweets
producidos
días
¿es
necesario
mirarlos
determinar
temas
discuten
día
¿es
necesario
mirar
tweets
determinar
sentimiento
cada
temas
fabricación
diferentes
tipos
datos
sensoriales
acústica
vibración
presión
corriente
voltaje
datos
controlador
disponibles
intervalos
tiempo
cortos
predecir
tiempo
inactividad
puede
necesario
examinar
datos
muestra
puede
ser
suficiente
big
data
puede
desglosar
varias
categorías
puntos
datos
datos
demográficos
psicográficos
comportamiento
transaccionales
grandes
conjuntos
puntos
datos
especialistas
marketing
pueden
crear
utilizar
segmentos
consumidores
personalizados
orientación
estratégica
realizado
trabajos
algoritmos
muestreo
big
data
desarrollado
formulación
teórica
muestreo
datos
twitter
116
​
hacia
mediados
2009
mundo
experimentó
pandemia
gripe
llamada
gripe
porcina
h1n1
web
google
flu
trends
117
​
intentó
predecirla
partir
resultados
búsquedas
google
flu
trends
usaba
datos
búsquedas
usuarios
contenían
síntomas
parecidos
enfermedad
gripe
agrupó
según
ubicación
fecha
pretendía
predecir
actividad
gripe
dos
semanas
antelación
sistemas
tradicionales
embargo
2013
descubrió
predijo
doble
visitas
médico
realidad
creadores
cometieron
dos
errores
nueva
herramienta
generado
muchísimo
interés
público
consultaba
curiosidad
necesidad
generó
ruido
información
b
algoritmos
predicción
buscadores
artículo
revista
science
2014
analizaron
errores
cometidos
google
flu
trends
``
querer
sustituir
técnicas
datos
masivos
métodos
tradicionales
probados
recolección
análisis
datos
vez
sólo
aplicar
dichas
técnicas
complemento
hizo
brittany
wenger
cloud4cancer
''
google
flu
trends
dejó
funcionar
118
​
concretamente
nueva
zelanda
119
​
cruzaron
datos
tendencias
gripe
google
datos
existentes
sistemas
salud
nacionales
comprobaron
alineados
gráficos
mostraron
correlación
búsquedas
síntomas
relacionados
gripe
extensión
pandemia
país
países
sistemas
predicción
desarrollados
pueden
beneficiarse
predicción
fiable
pública
abastecer
población
medidas
seguridad
oportunas
1853
1854
londres
epidemia
cólera
mató
miles
personas
médico
john
snow
estudió
registros
defunciones
descubrió
mayor
parte
casos
presentaron
barrio
específico
personas
bebido
agua
mismo
pozo
clausuraron
número
casos
comenzó
disminuir
120
​
2012
feria
ciencias
google
brittany
wenger
estudiante
18
años
presentó
proyecto
diseño
software
ayudar
diagnóstico
temprano
cáncer
mama
denominó
plataforma
cloud4cancer
utiliza
red
inteligencia
artificial
bases
datos
hospitales
diferenciar
muestra
tejido
benigno
tumor
maligno
sistema
inteligente
diseñado
wenger
distingue
segundos
dos
tipo
tumores
ingresando
plataforma
características
observadas
posible
sistema
aplique
adelante
padecimientos
leucemia
121
​
incrementar
seguridad
frente
ataques
propias
organizaciones
empresas
entorno
económico
propios
ministerios
defensa
entorno
ciberataques
contempla
utilidad
tecnologías
big
data
escenarios
vigilancia
seguridad
fronteras
lucha
terrorismo
crimen
organizado
fraude
planes
seguridad
ciudadana
planeamiento
táctico
misiones
inteligencia
militar
122
​
proyecto
aloja
123
​
sido
iniciado
apuesta
común
barcelona
supercomputing
center
bsc
microsoft
research
objetivo
proyecto
big
data
quiere
«
conseguir
optimización
automática
despliegues
hadoop
diferentes
infraestructuras
»
40
caso
específico
sostenibilidad
conservation
international
organización
propósito
concienciar
sociedad
cuidar
entorno
manera
responsable
sostenible
ayuda
plataforma
vertica
analytics
hp
situado
1000
cámaras
largo
dieciséis
bosques
cuatro
continentes
cámaras
incorporan
sensores
modo
cámara
oculta
graban
comportamiento
fauna
imágenes
datos
sensores
precipitaciones
temperatura
humedad
solar…
consiguen
información
cómo
cambio
climático
desgaste
tierra
afecta
comportamiento
desarrollo
124
​
críticas
paradigma
big
data
vienen
dos
formas
aquellas
cuestionan
implicaciones
enfoque
mismo
cuestionan
forma
realiza
actualmente
125
​
enfoque
crítica
campo
estudios
datos
críticos
``
problema
crucial
sabemos
microprocesos
empíricos
subyacentes
conducen
aparición
características
red
típicas
big
data
''
126
​
crítica
snijders
matzat
reips
señalan
menudo
hacen
suposiciones
fuertes
propiedades
matemáticas
pueden
reflejar
absoluto
realmente
sucediendo
nivel
microprocesos
mark
graham
criticado
ampliamente
afirmación
chris
anderson
macrodatos
marcarán
final
teoría
127
​
centrándose
particular
noción
macrodatos
siempre
deben
contextualizarse
contextos
sociales
económicos
políticos
128
​
incluso
empresas
invierten
sumas
ocho
nueve
cifras
obtener
información
transmisión
información
proveedores
clientes
menos
40
empleados
procesos
habilidades
suficientemente
maduros
hacerlo
superar
déficit
perspicacia
grandes
datos
importar
cuán
exhaustivos
bien
analizados
complementen
``
gran
juicio
''
según
artículo
harvard
business
review
129
​
misma
línea
señalado
decisiones
basadas
análisis
big
data
inevitablemente
``
informadas
mundo
pasado
mejor
casos
actualmente
''
alimentados
gran
cantidad
datos
experiencias
pasadas
algoritmos
pueden
predecir
desarrollo
futuro
si
futuro
similar
pasado
130
​
si
dinámica
sistemas
futuro
cambia
si
proceso
estacionario
pasado
puede
decir
futuro
hacer
predicciones
entornos
cambiantes
necesario
tener
conocimiento
profundo
dinámica
sistemas
requiere
teoría
respuesta
crítica
alemany
oliver
vayre
sugirieron
usar
``
razonamiento
abductivo
primer
paso
proceso
investigación
traer
contexto
huellas
digitales
consumidores
hacer
emerjan
nuevas
teorías
''
131
​
además
sugerido
combinar
enfoques
big
data
simulaciones
computadora
tales
modelos
basados
​​en
agentes
sistemas
complejos
modelos
basados
​​en
agentes
cada
vez
mejores
predecir
resultado
complejidades
sociales
escenarios
futuros
incluso
desconocidos
través
simulaciones
computadora
basan
colección
algoritmos
mutuamente
interdependientes
132
​
133
​
finalmente
uso
métodos
multivariantes
exploran
estructura
latente
datos
análisis
factorial
análisis
conglomerados
demostrado
ser
útiles
enfoques
analíticos
van
allá
enfoques
bi-variados
tablas
cruzadas
típicamente
empleados
conjuntos
datos
pequeños
salud
biología
enfoques
científicos
convencionales
basan
experimentación
enfoques
factor
limitante
información
relevante
puede
confirmar
refutar
hipótesis
inicial
134
​
ahora
acepta
nuevo
postulado
ciencias
biológicas
información
provista
datos
grandes
volúmenes
ómicas
hipótesis
previas
complementaria
veces
necesaria
enfoques
convencionales
basados
​​en
experimentación
135
​
enfoques
masivos
formulación
hipótesis
relevante
explicar
datos
factor
limitante
136
​
lógica
búsqueda
invierte
deben
considerar
límites
inducción
``
gloria
ciencia
escándalo
filosofía
''
c.
d.
broad
1926
defensores
privacidad
preocupados
amenaza
privacidad
representa
aumento
almacenamiento
integración
información
identificación
personal
paneles
expertos
publicado
varias
recomendaciones
políticas
adaptar
práctica
expectativas
privacidad
137
​
138
​
139
​
uso
indebido
big
data
varios
casos
medios
empresas
incluso
gobierno
permitido
abolición
confianza
casi
todas
instituciones
fundamentales
sostienen
sociedad
140
​
nayef
al-rodhan
sostiene
necesitará
nuevo
tipo
contrato
social
proteger
libertades
individuales
contexto
big
data
corporaciones
gigantes
poseen
grandes
cantidades
información
uso
big
data
debería
supervisarse
regularse
mejor
nivel
nacional
internacional
141
​
barocas
nissenbaum
argumentan
forma
proteger
usuarios
individuales
informando
tipos
información
recopila
quién
comparte
bajo
limitaciones
fines
142
​
modelo
v
big
data
concertante
centra
escalabilidad
computacional
carece
pérdida
torno
perceptibilidad
comprensibilidad
información
llevó
marco
cognitive
big
data
caracteriza
aplicación
big
data
acuerdo
143
​
grandes
conjuntos
datos
sido
analizados
máquinas
computación
siglo
incluida
analítica
censo
estadounidense
realizada
1890
máquinas
tarjetas
perforadas
ibm
computaron
estadísticas
incluían
medias
variaciones
poblaciones
continente
décadas
recientes
experimentos
científicos
cern
producido
datos
escalas
similares
``
grandes
datos
''
comerciales
actuales
embargo
experimentos
científicos
tendido
analizar
datos
utilizando
clusters
grids
especializados
computación
alto
rendimiento
supercomputación
lugar
nubes
computadoras
básicas
baratas
ola
comercial
actual
implica
diferencia
cultura
tecnología
ulf-dietrich
reips
uwe
matzat
escribieron
2014
big
data
convertido
``
moda
''
investigación
científica
investigadora
danah
boyd
expresado
preocupación
uso
big
data
ciencia
descuidando
principios
elegir
muestra
representativa
demasiado
preocupado
manejar
grandes
cantidades
datos
144
​
enfoque
puede
generar
sesgos
resultados
forma
u
integración
través
recursos
datos
heterogéneos
-algunos
pueden
considerarse
grandes
datos
no-
presenta
desafíos
logísticos
analíticos
formidables
investigadores
sostienen
tales
integraciones
probablemente
representen
nuevas
fronteras
prometedoras
ciencia
145
​
provocativo
artículo
``
preguntas
críticas
big
data
''
146
​
autores
titulan
big
data
parte
mitología
``
grandes
conjuntos
datos
ofrecen
forma
superior
inteligencia
conocimiento
...
aura
verdad
objetividad
precisión
``
usuarios
big
data
menudo
``
pierden
gran
volumen
números
''
``
trabajar
big
data
sigue
siendo
subjetivo
cuantifica
necesariamente
reclamo
cercano
verdad
objetiva
''
desarrollos
recientes
dominio
bi
informes
proactivos
apuntan
especialmente
mejoras
usabilidad
big
data
través
filtrado
automatizado
datos
correlaciones
útiles
147
​
análisis
big
data
suele
ser
profundo
comparación
análisis
conjuntos
datos
pequeños
194
proyectos
big
data
grandes
análisis
datos
desafío
extraer
transformar
cargar
parte
preprocesamiento
datos
148
​
big
data
palabra
moda
``
término
vago
''
149
​
150
​
mismo
tiempo
``
obsesión
''
empresarios
consultores
científicos
medios
comunicación
muestras
datos
grandes
google
flu
trends
generaron
buenas
predicciones
últimos
años
exageró
brotes
gripe
factor
dos
mismo
modo
premios
academia
predicciones
electorales
basadas
únicamente
twitter
menudo
objetivo
grandes
datos
menudo
presentan
mismos
desafíos
datos
pequeños
agregar
datos
resuelve
problemas
sesgo
puede
enfatizar
problemas
particular
fuentes
datos
twitter
representativas
población
general
resultados
extraídos
dichas
fuentes
pueden
dar
lugar
conclusiones
erróneas
google
translate
basa
análisis
estadístico
big
data
textos
hace
buen
trabajo
traducir
páginas
web
embargo
resultados
dominios
especializados
pueden
ser
dramáticamente
sesgados
lado
macrodatos
pueden
introducir
nuevos
problemas
problema
comparaciones
múltiples
prueba
simultánea
gran
conjunto
hipótesis
probablemente
produzca
resultados
falsos
erróneamente
parecen
significativos
ioannidis
argumentó
``
mayoría
resultados
investigación
publicados
falsos
''
151
​
debido
esencialmente
mismo
efecto
equipos
científicos
investigadores
realizan
cada
experimentos
decir
procesan
gran
cantidad
datos
científicos
aunque
big
data
probabilidad
resultado
``
significativo
''
falso
crece
rápidamente
incluso
publican
resultados
positivos
además
resultados
análisis
big
data
tan
buenos
modelo
basan
ejemplo
big
data
participó
intento
predecir
resultados
elecciones
presidenciales
ee
uu
2016
152
​
diversos
grados
éxito
forbes
predijo
``
si
usted
cree
análisis
big
data
hora
comenzar
planificar
presidencia
hillary
clinton
implica
''
153
​
hora
construir
big
data
debe
tener
cuenta
requisitos
seguridad
1.
acceso
autorización
granular
datos
granulación
quiere
decir
datos
cuales
acceso
autorización
alto
nivel
agrupados
otorgarán
función
precisa
oportuna
utilizará
datos
separados
punto
va
incluido
gobernabilidad
datos
debe
tener
saber
gobernabilidad
datos
refiere
datos
deben
autorizados
organizados
permisos
usuario
necesarios
base
datos
menor
número
posible
errores
manteniendo
mismo
tiempo
privacidad
seguridad
tener
efectivo
gobierno
datos
deberán
existir
controles
granulares
pueden
lograr
través
expresiones
control
acceso
expresiones
usan
agrupación
lógica
booleana
controlar
acceso
autorización
datos
flexibles
permisos
basados
roles
configuración
visibilidad
pueden
tener
diferentes
niveles
acceso
dar
seguridad
integrada
2.
seguridad
perimetral
protección
datos
autenticación
integrada
seguridad
perimetral
define
aquellos
elementos
sistemas
permiten
proteger
perímetros
instalaciones
sensibles
ser
atacados
sistemas
informáticos
intrusos
trata
primera
línea
defensa
reduce
muchísimo
riesgo
roben
datos
incluso
desaparezcan
seguridad
perimetral
protege
sistemas
debe
cumplir
cuatro
funciones
básicas
herramientas
pueden
utilizar
seguridad
perimetral
firewalls
pues
definen
mediante
política
acceso
tipo
tráfico
permite
deniega
red
sistemas
detección
prevención
intrusos
dispositivos
monitorizan
generan
alarmas
alertas
seguridad
honeypots
trata
trampa
atraer
analizar
ataques
bots
hackers
antispam
filtran
contenido
malicioso
entra
red
gobernabilidad
ocurre
seguridad
punto
final
cadena
importante
construir
buen
perímetro
colocar
cortafuego
alrededor
datos
integrados
sistemas
estándares
autenticación
existentes
trata
autenticación
importante
empresas
sincronicen
sistemas
probados
autenticación
trata
ver
cómo
integrarse
ldap
lightweight
directory
access
protocol
active
directory
servicios
directorio
puede
dar
soporte
herramientas
kerberos
soporte
autenticación
importante
crear
infraestructura
separada
sino
integrarla
estructura
existente
3.
encriptación
datos
siguiente
paso
después
proteger
perímetro
autenticar
acceso
granular
datos
otorgando
asegurarse
archivos
información
personalmente
identificable
encriptados
extremo
extremo
necesario
encriptar
datos
forma
independientemente
quién
acceso
puedan
ejecutar
análisis
necesiten
exponer
ninguno
datos
encriptación
procedimiento
mediante
archivos
cualquier
tipo
documento
vuelve
completamente
ilegibles
gracias
algoritmo
desordena
componentes
así
cualquier
persona
disponga
claves
correctas
podrá
acceder
información
contiene
4.
constante
auditoría
análisis
auditoría
proceso
implementado
auditores
sistemas
fin
auditar
accesos
datos
general
siguiendo
bien
metodología
basada
lista
contempla
puntos
quieren
comprobar
mediante
evaluación
riesgos
potenciales
concreto
realiza
examen
accesos
datos
almacenados
bases
datos
fin
poder
medir
monitorear
tener
constancia
accesos
información
almacenada
mismas
nivel
visibilidad
responsabilidad
cada
paso
proceso
permite
“
gobernar
''
datos
lugar
simplemente
establecer
políticas
controles
acceso
esperar
mejor
cómo
empresas
pueden
mantener
estrategias
actualizadas
entorno
forma
vemos
datos
tecnologías
utilizamos
administrarlos
analizarlos
cambiando
cada
día
fin
persigue
u
modo
seguridad
corporativa
auditoría
base
datos
facilita
herramientas
eficaces
conocer
forma
exacta
cuál
relación
usuarios
hora
acceder
bases
datos
incluyendo
actuaciones
deriven
generación
modificación
eliminación
datos
virtualización
big
data
forma
recopilar
información
múltiples
fuentes
mismo
lugar
ensamblaje
virtual
diferencia
métodos
mayoría
datos
permanecen
lugar
toman
bajo
demanda
directamente
sistema
origen
154
​
gestione
cuenta
acceda
contenido
personalizado
regístrese
obtener
cuenta
oracle
acceda
panel
nube
gestione
pedidos
regístrese
prueba
gratuita
¿qué
big
data
2:32
¿qué
exactamente
big
data
entender
significa
realmente
``
big
data
''
resulta
útil
conocer
ciertos
antecedentes
históricos
continuación
ofrecemos
definición
gartner
aproximadamente
2001
continúa
siendo
definición
referencia
big
data
datos
contienen
mayor
variedad
presentan
volúmenes
crecientes
velocidad
superior
conoce
``
tres
v
''
dicho
modo
big
data
formado
conjuntos
datos
mayor
tamaño
complejos
especialmente
procedentes
nuevas
fuentes
datos
conjuntos
datos
tan
voluminosos
software
procesamiento
datos
convencional
sencillamente
puede
gestionarlos
embargo
volúmenes
masivos
datos
pueden
utilizarse
abordar
problemas
empresariales
sido
posible
solucionar
cantidad
datos
importa
big
data
deberá
procesar
grandes
volúmenes
datos
estructurados
baja
densidad
puede
tratarse
datos
valor
desconocido
feeds
datos
twitter
flujos
clics
página
web
aplicación
móviles
equipo
sensores
organizaciones
puede
suponer
decenas
terabytes
datos
incluso
cientos
petabytes
velocidad
ritmo
reciben
datos
posiblemente
aplica
alguna
acción
mayor
velocidad
datos
normalmente
transmite
directamente
memoria
vez
escribirse
disco
productos
inteligentes
habilitados
internet
funcionan
tiempo
real
prácticamente
tiempo
real
requieren
evaluación
actuación
tiempo
real
variedad
hace
referencia
diversos
tipos
datos
disponibles
tipos
datos
convencionales
estructurados
podían
organizarse
claramente
base
datos
relacional
auge
big
data
datos
presentan
nuevos
tipos
datos
estructurados
tipos
datos
estructurados
semiestructurados
texto
audio
vídeo
requieren
preprocesamiento
adicional
poder
obtener
significado
habilitar
metadatos
últimos
años
surgido
``
dos
v
''
valor
veracidad
datos
poseen
valor
intrínseco
embargo
ninguna
utilidad
dicho
valor
descubre
resulta
igualmente
importante
¿cuál
veracidad
datos
cuánto
puede
confiar
hoy
día
big
data
convertido
activo
crucial
piense
mayores
empresas
tecnológicas
mundo
gran
parte
valor
ofrecen
procede
datos
analizan
constantemente
generar
mayor
eficiencia
desarrollar
nuevos
productos
avances
tecnológicos
recientes
reducido
exponencialmente
coste
almacenamiento
computación
datos
haciendo
almacenar
datos
resulte
fácil
barato
nunca
actualmente
mayor
volumen
big
data
barato
accesible
puede
tomar
decisiones
empresariales
acertadas
precisas
identificar
valor
big
data
pasa
solo
analizarlo
ventaja
misma
trata
proceso
descubrimiento
requiere
analistas
usuarios
empresariales
ejecutivos
planteen
preguntas
correctas
identifiquen
patrones
tomen
decisiones
informadas
predigan
comportamientos
¿cómo
llegado
aquí
descargue
informe
gratuito
si
bien
concepto
``
big
data
''
mismo
relativamente
nuevo
orígenes
grandes
conjuntos
datos
remontan
décadas
1960
1970
sitúan
albores
universo
primeros
centros
datos
desarrollo
bases
datos
relacionales
alrededor
2005
gente
empezó
darse
cuenta
cantidad
datos
generaban
usuarios
través
facebook
youtube
servicios
online
mismo
año
desarrollaría
hadoop
marco
código
abierto
creado
específicamente
almacenar
analizar
grandes
conjuntos
datos
época
empezaría
adquirir
popularidad
nosql
desarrollo
marcos
código
abierto
tales
hadoop
recientemente
spark
esencial
crecimiento
big
data
pues
hacían
big
data
resultase
fácil
usar
barato
almacenar
años
siguientes
volumen
big
data
disparado
usuarios
continúan
generando
enormes
cantidades
datos
ahora
humanos
únicos
hacen
llegada
internet
cosas
iot
mayor
número
objetos
dispositivos
conectados
internet
generan
datos
patrones
uso
clientes
rendimiento
productos
surgimiento
aprendizaje
automático
producido
aún
datos
aunque
big
data
llegado
lejos
utilidad
hecho
empezar
cloud
computing
ampliado
aún
posibilidades
big
data
nube
ofrece
escalabilidad
realmente
elástica
desarrolladores
pueden
simplemente
agilizar
clústeres
ad
hoc
probar
subconjunto
datos
ventajas
big
data
analítica
datos
descubra
recursos
big
data
big
data
puede
ayudarle
abordar
serie
actividades
empresariales
experiencia
cliente
analítica
continuación
recopilamos
puede
encontrar
casos
uso
soluciones
oracle
big
data
desarrollo
productos
empresas
netflix
procter
gamble
usan
big
data
prever
demanda
clientes
construyen
modelos
predictivos
nuevos
productos
servicios
clasificando
atributos
clave
productos
anteriores
actuales
modelando
relación
dichos
atributos
éxito
comercial
ofertas
además
p
g
utiliza
datos
analítica
grupos
interés
redes
sociales
mercados
prueba
avances
salida
tiendas
planificar
producir
lanzar
nuevos
productos
mantenimiento
predictivo
factores
capaces
predecir
fallos
mecánicos
pueden
profundamente
ocultos
datos
estructurados
año
equipo
marca
modelo
máquina
datos
estructurados
cubren
millones
entradas
registros
datos
sensores
mensajes
error
temperaturas
motor
analizar
indicadores
problemas
potenciales
produzcan
organizaciones
pueden
implantar
mantenimiento
forma
rentable
optimizar
tiempo
servicio
componentes
equipos
experiencia
cliente
carrera
conseguir
clientes
comenzado
disponer
vista
clara
experiencia
cliente
posible
nunca
big
data
permite
recopilar
datos
redes
sociales
visitas
páginas
web
registros
llamadas
fuentes
datos
mejorar
experiencia
interacción
así
maximizar
valor
ofrecido
empiece
formular
ofertas
personalizadas
reducir
tasas
abandono
clientes
gestionar
incidencias
manera
proactiva
fraude
conformidad
seguridad
refiere
enfrenta
simples
piratas
informáticos
deshonestos
sino
equipos
completos
expertos
contextos
seguridad
requisitos
conformidad
constante
evolución
big
data
ayuda
identificar
patrones
datos
pueden
ser
indicativos
fraude
tiempo
concentra
grandes
volúmenes
información
agilizar
generación
informes
normativos
aprendizaje
automático
aprendizaje
automático
actualmente
tema
gran
actualidad
datos
—concretamente
big
data—
motivos
así
ahora
lugar
programarse
máquinas
pueden
aprender
posible
gracias
disponibilidad
big
data
crear
modelos
aprendizaje
automático
eficiencia
operativa
puede
eficiencia
operativa
aspecto
destacado
titulares
área
big
data
mayor
impacto
big
data
permite
analizar
evaluar
producción
opinión
clientes
devoluciones
factores
reducir
situaciones
falta
stock
anticipar
demanda
futura
big
data
puede
utilizarse
mejorar
toma
decisiones
función
demanda
mercado
cada
momento
impulso
innovación
big
data
puede
ayudarle
innovar
mediante
estudio
interdependencias
seres
humanos
instituciones
entidades
procesos
posteriormente
mediante
determinación
nuevas
formas
usar
dicha
información
utilice
perspectivas
ofrecen
datos
mejorar
decisiones
financieras
consideraciones
planificación
estudie
tendencias
desean
clientes
ofrecer
nuevos
productos
servicios
implante
políticas
precios
dinámicas
posibilidades
infinitas
si
bien
cierto
big
data
promete
enfrenta
desafíos
primer
lugar
big
data
caracteriza
gran
tamaño
aunque
desarrollado
nuevas
tecnologías
almacenamiento
datos
volumen
datos
duplica
tamaño
cada
dos
años
aproximadamente
organizaciones
continúan
esforzándose
mantener
ritmo
crecimiento
datos
encontrar
formas
almacenarlos
eficazmente
basta
almacenar
datos
ser
algún
valor
datos
deben
poder
utilizarse
depende
conservación
disponer
datos
limpios
—es
decir
datos
relevantes
cliente
organizados
tal
modo
permitan
análisis
significativo—
requiere
gran
cantidad
trabajo
científicos
datos
dedican
50
80
ciento
tiempo
seleccionar
preparar
datos
puedan
utilizarse
último
tecnología
big
data
cambia
ritmo
rápido
hace
años
apache
hadoop
tecnología
conocida
utilizada
gestionar
big
data
tarde
2014
entraría
juego
apache
spark
hoy
día
enfoque
óptimo
parece
ser
combinación
ambos
marcos
mantenerse
día
cuanto
tecnología
big
data
supone
desafío
constante
descubra
recursos
big
data
big
data
aporta
nuevas
perspectivas
abren
paso
nuevas
oportunidades
modelos
negocio
iniciarse
ello
requiere
tres
acciones
clave
big
data
concentra
datos
numerosas
fuentes
aplicaciones
distintas
mecanismos
integración
datos
convencionales
tales
etl
extract
transform
load
extraer
transformar
cargar
generalmente
altura
dicha
tarea
analizar
conjuntos
big
data
terabytes
incluso
petabytes
tamaño
requiere
nuevas
estrategias
tecnologías
integración
necesario
incorporar
datos
procesarlos
asegurarse
formateados
disponibles
tal
forma
analistas
empresariales
puedan
empezar
utilizarlos
big
data
requiere
almacenamiento
solución
almacenamiento
puede
residir
nube
on
premise
ambas
puede
almacenar
datos
cualquier
forma
desee
incorporar
requisitos
procesamiento
preferencia
motores
procesamiento
necesarios
dichos
conjuntos
datos
on-demand
muchas
personas
eligen
solución
almacenamiento
función
dónde
residan
datos
cada
momento
nube
aumentando
progresivamente
popularidad
compatible
requisitos
tecnológicos
actuales
permite
incorporar
recursos
medida
necesita
inversión
big
data
rentabiliza
cuanto
analizan
utilizan
datos
adquiera
nueva
claridad
análisis
visual
diversos
conjuntos
datos
continúe
explorando
datos
realizar
nuevos
descubrimientos
comparta
hallazgos
personas
construya
modelos
datos
aprendizaje
automático
inteligencia
artificial
ponga
datos
trabajar
ayudarle
transición
big
data
recopilado
serie
mejores
prácticas
debe
tener
cuenta
continuación
detallamos
pautas
crear
éxito
base
big
data
disponibilidad
conjuntos
datos
amplios
permite
realizar
nuevos
hallazgos
tal
efecto
importante
basar
nuevas
inversiones
habilidades
organización
infraestructura
marcado
contexto
empresarial
garantizar
constancia
financiación
inversión
proyectos
determinar
si
encuentra
camino
correcto
pregúntese
medida
big
data
respalda
habilita
principales
prioridades
empresariales
ejemplos
incluyen
entender
cómo
filtrar
registros
web
comprender
comportamiento
comercio
electrónico
extraer
sentimiento
redes
sociales
interacciones
atención
cliente
así
entender
métodos
correlación
estadística
relevancia
datos
clientes
productos
fabricación
ingeniería
mayores
obstáculos
big
data
escasez
habilidades
puede
mitigar
riesgo
asegurándose
incorporar
programa
administración
tecnologías
consideraciones
decisiones
relativas
big
data
normalizar
enfoque
permitirá
gestionar
costes
aprovechar
recursos
organizaciones
implanten
soluciones
estrategias
big
data
deben
evaluar
necesidades
habilidades
forma
temprana
frecuente
identificar
manera
proactiva
posibles
carencias
habilidades
puede
lograrse
mediante
impartición
formación
formación
cruzada
recursos
existentes
contratación
nuevos
recursos
uso
empresas
consultoría
utilice
enfoque
basado
centro
excelencia
compartir
conocimientos
supervisar
control
gestionar
comunicaciones
proyectos
si
big
data
inversión
nueva
expansión
costes
directos
indirectos
pueden
distribuirse
toda
empresa
utilizar
enfoque
puede
contribuir
incrementar
capacidades
big
data
madurez
conjunto
arquitectura
información
forma
sistemática
estructurada
analizar
big
data
forma
aislada
duda
aporta
valor
embargo
puede
obtener
perspectiva
empresarial
aún
valiosa
relacionando
integrando
big
data
baja
densidad
datos
estructurados
usa
actualmente
si
recopilando
big
data
clientes
productos
equipos
ambientales
objetivo
añadir
puntos
datos
relevantes
resúmenes
maestros
analíticos
permitirá
obtener
mejores
conclusiones
ejemplo
existe
diferencia
distinguir
percepción
clientes
solo
mejores
clientes
consideran
big
data
constituye
extensión
integral
capacidades
existentes
inteligencia
empresarial
plataforma
almacenamiento
datos
arquitectura
información
cuenta
modelos
procesos
analíticos
big
data
pueden
ser
humanos
automáticos
capacidades
análisis
big
data
incluyen
estadísticas
análisis
especiales
semántica
detección
interactiva
visualización
mediante
uso
modelos
analíticos
puede
relacionar
distintos
tipos
fuentes
datos
realizar
asociaciones
hallazgos
significativos
concepto
``
hallazgo
''
implica
datos
siempre
obtienen
directamente
ocasiones
siquiera
sabemos
buscando
esperar
dirección
equipos
deben
respaldar
“
falta
dirección
”
“
falta
claridad
requisitos.
”
mismo
tiempo
importante
analistas
científicos
datos
colaboren
estrechamente
empresa
entender
principales
necesidades
carencias
conocimientos
empresa
incorporar
estudio
interactivo
datos
experimentación
algoritmos
estadísticos
necesita
contar
áreas
trabajo
alto
rendimiento
asegúrese
entornos
pruebas
sandbox
potencia
necesaria
correctamente
gobernados
usuarios
procesos
big
data
requieren
acceso
amplia
variedad
recursos
experimentación
reiterativa
ejecución
tareas
producción
solución
big
data
incluye
ámbitos
datos
incluidas
transacciones
datos
principales
datos
referencia
datos
resumidos
entornos
pruebas
sandboxes
analíticos
deben
crearse
on-demand
gestión
recursos
fundamental
garantizar
control
flujo
datos
incluido
procesamiento
previo
posterior
integración
resumen
dentro
base
datos
creación
modelos
analíticos
disponer
estrategia
bien
definida
aprovisionamiento
seguridad
nube
pública
privada
fundamental
respaldar
requisitos
cambiantes
comuníquese
experto
ventas
seguir
siga
oracle
corporate
ingreso
bienvenido
editar
perfil
salir
worldwide
sites
worldwide
contacts
if
you
do
n't
find
your
country
in
the
list
see
our
worldwide
contacts
list
soluciones
industria
watch
the
video
read
the
story
soluciones
empresariales
productos
sas
for
...
built
for
analytics
innovation
get
details
tecnología
ia
abre
nuevas
posiblidades
conozca
comience
aprender
sas
entrenamiento
certificación
estudiantes
educadores
librerí­a
documentacíon
centro
recursos
áreas
foco
documentación
técnica
soporte
conocimiento
base
soporte
producto
sas
services
downloads
hot
fixes
sas
administrators
manage
your
tracks
product
resources
get
the
help
you
need
to
resolve
problems
get
help
explore
documentation
for
sas
software
products
read
documentation
customer
stories
featured
customers
encuentre
partner
mire
partners
oro
platino
busque
listado
acerca
programa
lea
niveles
programa
partners
oportunidades
canal
inscríbase
partnernet
obtenga
recursos
entrenamiento
marketing
membresías
partners
actuales
platinum
partners
acerca
sas
empleo
noticias
eventos
office
information
colombia
bogotá
edificio
torre
cusezar
calle
116
7-15
interior
2
oficina
902
tel
571
658-0888
ubicación
colombia
medellín
calle
7
sur
42-70
torre
2
oficina
916
edificio
forum
tel
4
460
1585
86
ecuador
quito
iñaquito
36
naciones
unidas
edif
platinum
g
piso
9.
big
data
término
describe
gran
volumen
datos
–
estructurados
estructurados
–
inundan
empresa
días
cantidad
datos
importante
importa
organizaciones
hacen
datos
big
data
puede
ser
analizado
obtener
insights
conlleven
mejores
decisiones
acciones
negocios
estratégicas
aunque
término
“
big
data
”
relativamente
nuevo
acción
recopilar
almacenar
grandes
cantidades
información
posterior
análisis
viene
realizando
hace
años
concepto
cobró
impulso
principios
década
2000
analista
industria
doug
laney
articuló
definición
ahora
popular
big
data
tres
vs
volumen
organizaciones
recopilan
datos
diversas
fuentes
incluyendo
transacciones
comerciales
medios
sociales
información
sensores
transmite
máquina
pasado
almacenarlos
sido
problema
–
nuevas
tecnologías
hadoop
aligerado
tarea
velocidad
datos
transmiten
velocidad
precedentes
deben
distribuir
manera
oportuna
etiquetas
fid
sensores
medición
inteligente
crean
necesidad
distribuir
torrentes
datos
casi
tiempo
real
variedad
datos
vienen
toda
clase
formatos
–
datos
numéricos
estructurados
bases
datos
tradicionales
documentos
texto
estructurados
correo
electrónico
video
audio
datos
teletipo
bursátil
transacciones
financieras
sas
consideramos
dos
dimensiones
trata
big
data
variedad
además
velocidades
variedades
datos
cada
vez
mayores
flujos
datos
pueden
ser
inconsistentes
picos
periódicos
¿es
alguna
tendencia
medios
sociales
cargas
datos
máximas
diarias
temporada
desencadenadas
eventos
pueden
ser
difíciles
controlar
aún
datos
estructurados
complejidad
datos
actualidad
provienen
múltiples
fuentes
hace
difícil
vincular
empatar
depurar
transformar
datos
diferentes
sistemas
embargo
necesario
conectar
correlacionar
relaciones
jerarquías
múltiples
vínculos
datos
datos
pueden
salir
control
segundo
cantidad
datos
crean
almacenan
nivel
global
casi
inconcebible
continúa
aumento
quiere
decir
aún
mayor
potencial
obtener
insights
clave
información
negocios
–
aunque
sólo
analice
realidad
pequeño
porcentaje
datos
¿qué
significa
empresas
¿cómo
pueden
hacer
mejor
uso
información
cruda
reciben
organizaciones
cada
día
importancia
big
data
gira
torno
cuántos
datos
usted
sino
hace
puede
tomar
datos
cualquier
fuente
analizarlos
hallar
respuestas
hagan
posibles
1
reducciones
costos
2
reducciones
tiempo
3
desarrollo
nuevos
productos
soluciones
optimizadas
4
toma
decisiones
inteligente
combina
big
data
analítica
poderosa
pueden
realizar
tareas
relacionadas
negocios
tales
mantenerse
relevante
integración
datos
necesita
trabajar
diferentes
tipos
fuentes
datos
mismo
tiempo
operar
diferentes
latencias
–
tiempo
real
streaming
aprenda
cómo
evolucionado
di
satisfacer
requisitos
modernos
lea
documento
documento
examina
forma
profesional
negocios
erudito
formación
técnica
puede
entender
cómo
usar
hadoop
–
cómo
afectará
éste
entornos
datos
empresariales
próximos
años
lea
resumen
¿es
término
``
data
lake
''
simple
exageración
marketing
¿o
nuevo
nombre
almacén
datos
phil
simon
deja
claro
lago
datos
cómo
funciona
cuándo
podría
usted
necesitar
lea
artículo
sas
provides
everything
you
need
to
get
valuable
insights
from
all
that
data
learn
more
about
big
data
solutions
from
sas
grandes
cantidades
información
proveniente
fuentes
incontables
bancos
enfrentan
labor
hallar
formas
nuevas
innovadoras
manipular
big
data
aunque
importante
entender
clientes
elevar
nivel
satisfacción
igualmente
importante
minimizar
riesgo
fraude
mismo
tiempo
mantener
cumplimiento
regulaciones
big
data
genera
grandes
insights
requiere
instituciones
financieras
mantengan
paso
adelante
juego
analítica
avanzada
educadores
equipados
insights
basados
datos
pueden
producir
impacto
importante
sistemas
escolares
estudiantes
currículos
mediante
análisis
big
data
pueden
identificar
estudiantes
riesgo
asegurarse
estudiantes
progreso
adecuado
pueden
implementar
mejor
sistema
evaluación
soporte
maestros
directores
dependencias
gobierno
pueden
aprovechar
aplicar
analítica
big
data
ganan
terreno
importante
trata
gestionar
servicios
públicos
operar
dependencias
solucionar
congestiones
tráfico
prevenir
actividad
criminal
aunque
muchas
ventajas
usar
big
data
gobiernos
deben
hacer
frente
aspectos
transparencia
privacidad
registros
pacientes
planes
tratamiento
información
prescripciones
trata
atención
salud
necesita
hacer
rapidez
precisión
–
casos
transparencia
suficiente
satisfacer
estrictas
regulaciones
industria
big
data
utiliza
manera
eficiente
proveedores
atención
salud
pueden
descubrir
insights
ocultos
mejoran
atención
pacientes
equipados
insights
big
data
puede
proveer
fabricantes
pueden
elevar
calidad
producción
mismo
tiempo
minimizar
desperdicio
–
procesos
clave
mercado
altamente
competitivo
actualidad
cada
vez
fabricantes
trabajan
cultura
basada
analítica
significa
pueden
resolver
problemas
menos
tiempo
tomar
decisiones
negocios
ágiles
generación
relaciones
clientes
fundamental
industria
minorista
–
mejor
forma
lograrlo
través
gestión
big
data
minoristas
necesitan
conocer
mejor
manera
vender
productos
clientes
forma
eficiente
procesar
transacciones
forma
estratégica
recuperar
clientes
perdido
big
data
mantiene
corazón
todas
cosas
compañía
muchas
partes
piezas
constantemente
movimiento
ups
almacena
gran
cantidad
datos
–
cuales
provienen
sensores
vehículos
datos
sólo
monitorean
desempeño
diario
sino
impulsaron
rediseño
importante
estructuras
rutas
siguen
conductores
ups
iniciativa
denominó
orion
on-road
integration
optimization
and
navigation
probablemente
mayor
proyecto
investigación
operaciones
mundo
éste
apoya
mayormente
datos
mapas
línea
reconfigurar
recolecciones
entregas
conductor
tiempo
real
proyecto
generó
ahorros
8.4
millones
galones
combustible
tras
recortar
85
millones
millas
rutas
diarias
ups
calcula
ahorrar
tan
sólo
milla
diaria
conductor
ahorra
compañía
30
millones
dólares
modo
ahorros
globales
sustanciales
sas
makes
it
easy
to
understand
what
your
data
to
tell
you
interactively
explore
billions
of
rows
of
data
in
seconds
learn
more
about
big
data
solutions
from
sas
datos
streaming
categoría
incluye
datos
llegan
sistemas
red
dispositivos
conectados
puede
analizar
datos
conforme
van
llegando
tomar
decisiones
datos
conservar
cuáles
conservar
cuáles
requieren
análisis
fondo
datos
medios
sociales
datos
interacciones
sociales
conjunto
información
cada
vez
atractivo
particular
funciones
marketing
ventas
soporte
menudo
encuentran
formas
estructuradas
semiestructuradas
modo
presentan
reto
único
trata
consumo
análisis
fuentes
públicamente
disponibles
disponibles
cantidades
masivas
datos
través
fuentes
datos
abiertas
portal
data.gov
gobierno
unidos
cia
world
factbook
open
data
portal
unión
europea
cómo
almacenarlos
gestionarlos
aunque
almacenaje
sido
problema
hace
años
ahora
existen
opciones
económicas
almacenar
datos
si
ésa
mejor
estrategia
negocio
tantos
analizar
organizaciones
excluyen
datos
análisis
posible
tecnologías
alto
desempeño
actualidad
computación
red
distribuida
analítica
memoria
enfoque
consiste
determinar
principio
datos
relevantes
analizarlos
cómo
utilizar
insights
usted
descubre
cuantos
conocimientos
mayor
confianza
tomar
decisiones
negocios
decisión
inteligente
contar
estrategia
abundancia
información
mano
conéctese
soporte
clientes
insights
on
quick
links
privacidad
términos
uso
©
2019
sas
institute
inc.
all
rights
reserved
sas
dispone
políticas
gestión
calidad
medio
ambiente
seguridad
información
disposición
partes
interesadas
pertinentes
back
to
top
inicio
empresa
blog
contacto
big
data
término
describe
gran
volumen
datos
estructurados
estructurados
inundan
negocios
cada
día
cantidad
datos
importante
importa
big
data
organizaciones
hacen
datos
big
data
puede
analizar
obtener
ideas
conduzcan
mejores
decisiones
movimientos
negocios
estratégicos
hablamos
big
data
referimos
conjuntos
datos
combinaciones
conjuntos
datos
cuyo
tamaño
volumen
complejidad
variabilidad
velocidad
crecimiento
velocidad
dificultan
captura
gestión
procesamiento
análisis
mediante
tecnologías
herramientas
convencionales
tales
bases
datos
relacionales
estadísticas
convencionales
paquetes
visualización
dentro
tiempo
necesario
útiles
aunque
tamaño
utilizado
determinar
si
conjunto
datos
determinado
considera
big
data
firmemente
definido
sigue
cambiando
tiempo
mayoría
analistas
profesionales
actualmente
refieren
conjuntos
datos
van
30-50
terabytes
varios
petabytes
naturaleza
compleja
big
data
debe
principalmente
naturaleza
estructurada
gran
parte
datos
generados
tecnologías
modernas
web
logs
identificación
radiofrecuencia
rfid
sensores
incorporados
dispositivos
maquinaria
vehículos
búsquedas
internet
redes
sociales
facebook
computadoras
portátiles
teléfonos
inteligentes
teléfonos
móviles
dispositivos
gps
registros
centros
llamadas
mayoría
casos
fin
utilizar
eficazmente
big
data
debe
combinarse
datos
estructurados
normalmente
base
datos
relacional
aplicación
comercial
convencional
erp
enterprise
resource
planning
crm
customer
relationship
management
hace
big
data
tan
útil
muchas
empresas
hecho
proporciona
respuestas
muchas
preguntas
empresas
siquiera
sabían
palabras
proporciona
punto
referencia
cantidad
tan
grande
información
datos
pueden
ser
moldeados
probados
cualquier
manera
empresa
considere
adecuada
hacerlo
organizaciones
capaces
identificar
problemas
forma
comprensible
recopilación
grandes
cantidades
datos
búsqueda
tendencias
dentro
datos
permiten
empresas
muevan
rápidamente
problemas
manera
eficiente
permite
eliminar
áreas
problemáticas
problemas
acaben
beneficios
reputación
análisis
big
data
ayuda
organizaciones
aprovechar
datos
utilizarlos
identificar
nuevas
oportunidades
vez
conduce
movimientos
negocios
inteligentes
operaciones
eficientes
mayores
ganancias
clientes
felices
empresas
éxito
big
data
consiguen
valor
siguientes
formas
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'86fed0bf-49ab-4743-a270-aff8e5706334
ejemplo
especiales
características
big
data
hacen
calidad
datos
enfrente
múltiples
desafíos
trata
conocidas
5
vs
volumen
velocidad
variedad
veracidad
valor
definen
problemática
big
data
5
características
big
data
provocan
empresas
problemas
extraer
datos
reales
alta
calidad
conjuntos
datos
tan
masivos
cambiantes
complicados
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'e3479b19-8160-4c78-bc0e-1bccf3db54ab
llegada
big
data
mediante
etl
podíamos
cargar
información
estructurada
almacenada
sistema
erp
crm
ejemplo
ahora
podemos
cargar
información
adicional
encuentra
dentro
dominios
empresa
comentarios
likes
redes
sociales
resultados
campañas
marketing
datos
estadísticos
terceros
etc
datos
ofrecen
información
ayuda
saber
si
productos
servicios
funcionando
bien
contrario
problemas
desafíos
enfrenta
calidad
datos
big
data
tantas
fuentes
tipos
datos
estructuras
complejas
dificultad
integración
datos
aumenta
fuentes
datos
big
data
amplias
tipos
datos
solo
20
información
estructurada
puede
provocar
errores
si
acometemos
proyecto
calidad
datos
visto
volumen
datos
enorme
complica
ejecución
proceso
calidad
datos
dentro
tiempo
razonable
difícil
recolectar
limpiar
integrar
obtener
datos
alta
calidad
forma
rápida
necesita
tiempo
transformar
tipos
estructurados
tipos
estructurados
procesar
datos
datos
cambian
rápidamente
hace
validez
corta
solucionarlo
necesitamos
poder
procesamiento
alto
si
hacemos
bien
procesamiento
análisis
basado
datos
puede
producir
conclusiones
erróneas
pueden
llevar
cometer
errores
toma
decisiones
1987
organización
internacional
normalización
iso
publicó
normas
iso
9000
garantizar
calidad
productos
servicios
embargo
estudio
estándares
calidad
datos
comenzó
años
noventa
2011
iso
publicó
normas
calidad
datos
iso
8000.
normas
necesitan
madurar
perfeccionarse
además
investigación
calidad
datos
big
data
comenzado
hace
apenas
resultados
calidad
datos
big
data
clave
solo
poder
obtener
ventajas
competitivas
sino
impedir
incurramos
graves
errores
estratégicos
operacionales
basándonos
datos
erróneos
consecuencias
pueden
llegar
ser
graves
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'e38d631a-b512-40d8-b625-fcf132a6c7ee
gobernabilidad
significa
asegurarse
datos
autorizados
organizados
permisos
usuario
necesarios
base
datos
menor
número
posible
errores
manteniendo
mismo
tiempo
privacidad
seguridad
parece
equilibrio
fácil
conseguir
realidad
dónde
cómo
datos
alojan
procesan
constante
movimiento
continuación
veremos
pasos
recomendados
crear
plan
data
governance
big
data
puede
tener
gobierno
datos
efectivo
controles
granulares
pueden
lograr
controles
granulares
través
expresiones
control
acceso
expresiones
usan
agrupación
lógica
booleana
controlar
acceso
autorización
datos
flexibles
permisos
basados
roles
configuraciones
visibilidad
nivel
bajo
protegen
datos
confidenciales
ocultándolos
parte
superior
contratos
confidenciales
científicos
datos
analistas
bi
puede
hacer
capacidades
enmascaramiento
datos
diferentes
vistas
bloquean
datos
bruto
posible
gradualmente
proporciona
acceso
parte
superior
da
administradores
mayor
visibilidad
pueden
tener
diferentes
niveles
acceso
da
seguridad
integrada
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'62eb0514-cb6f-4f4e-aa46-3e4a920648a7
gobernabilidad
ocurre
seguridad
punto
final
cadena
importante
construir
buen
perímetro
colocar
cortafuegos
alrededor
datos
integrados
sistemas
estándares
autenticación
existentes
trata
autenticación
importante
empresas
sincronicen
sistemas
probados
autenticación
trata
ver
cómo
integrarse
ldap
lightweight
directory
access
protocol
active
directory
servicios
directorio
puede
dar
soporte
herramientas
kerberos
soporte
autenticación
importante
crear
infraestructura
separada
sino
integrarla
estructura
existente
siguiente
paso
después
proteger
perímetro
autenticar
acceso
granular
datos
otorgando
asegúrese
archivos
información
personalmente
identificable
pii
encriptados
tokenizados
extremo
extremo
pipeline
datos
vez
superado
perímetro
acceso
sistema
proteger
datos
pii
extremadamente
importante
necesario
encriptar
datos
forma
independientemente
quién
acceso
puedan
ejecutar
análisis
necesiten
exponer
ninguno
datos
estrategia
funciona
auditoría
nivel
visibilidad
responsabilidad
cada
paso
proceso
permite
``
gobernar
''
datos
lugar
simplemente
establecer
políticas
controles
acceso
esperar
mejor
cómo
empresas
pueden
mantener
estrategias
actualizadas
entorno
forma
vemos
datos
tecnologías
utilizamos
administrarlos
analizarlos
cambiando
cada
día
infancia
big
data
iot
internet
cosas
fundamental
poder
rastrear
acceso
reconocer
patrones
datos
auditoría
análisis
pueden
ser
tan
simples
seguimiento
archivos
javascript
object
notation
json
última
instancia
responsable
supervisar
estrategia
administración
datos
empresariales
debe
pensar
detalles
acceso
granular
autenticación
seguridad
cifrado
auditoría
debe
detenerse
ahí
bien
debe
pensar
cómo
cada
componentes
integra
arquitectura
datos
global
debe
pensar
cómo
infraestructura
va
necesitar
ser
escalable
segura
recolección
datos
almacenamiento
bi
analítica
servicios
terceros
gobernanza
datos
acerca
repensar
estrategia
ejecución
propia
tecnología
va
allá
conjunto
reglas
seguridad
arquitectura
única
crean
roles
sincronizan
través
toda
plataforma
todas
herramientas
aportan
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'6524d95b-f666-44a9-8328-67b2fa5ed72b
hbspt.cta._relativeurls=true
hbspt.cta.load
239039
'da1e3191-ca99-4b3f-962f-be97ee060673
