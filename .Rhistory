<<<<<<< HEAD
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word)%>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word)%>%
count(book, word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
slice(1:10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder_within(bigram, n, book), n, fill = book)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
scale_x_reordered() + xlab("bigramas") + labs(title ="Frecuencias de bigramas top 10")+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
series
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
frequency
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
frequency
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
win.graph()
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
win.graph()
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "red", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words),
p_value = cor.test(book_words, all_words)$p.value)
frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words))
core <- frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words))
library(Kableextra)
library(KablExtra)
library(kablExtra)
library(kableExtra)
kable(core)
kable(core, "markdown")
core <- frequency %>%
group_by(book) %>%
summarize(correlacion = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(term =book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(término =book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
library(igraph)
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
library(ggplot2) #
library(dplyr)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(stringr)
library(tm)
library(forcats)
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(estadistica),
text = estadistica) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
#correlacion
word_cor %>%
filter(item1 == "estadística") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(big_data),
text = big_data) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
#correlacion
word_cor %>%
filter(item1 == "big") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#analitica de datos
(ps_words <- tibble(chapter = seq_along(Analitica_de_datos),
text = Analitica_de_datos) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
word_cor %>%
filter(item1 == "analítica") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
new_x <- paste(x, within, sep = sep)
stats::reorder(new_x, by, FUN = fun)
}
#' @rdname reorder_within
#' @export
scale_x_reordered <- function(..., sep = "___") {
reg <- paste0(sep, ".+$")
ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
#' @rdname reorder_within
#' @export
scale_y_reordered <- function(..., sep = "___") {
reg <- paste0(sep, ".+$")
ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}
######## para un ejemplo #############
hh <- as.matrix(ofertas_ciencia_de_datos[1])
removeWords(hh,stopwords("spanish"))
removePunctuation(hh)
removeNumbers(hh)
stripWhitespace(hh)
hh <- tibble(hh)
hh %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
library(ggplot2) #
library(dplyr)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(stringr)
library(tm)
library(forcats)
# Cargar terminologias
load(file = "textominado.RData")
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
## contando palabras mas comunes en todo el texto de la serie SIN FILTRO
series %>%
count(word, sort = TRUE)
#### Definimos las stop words en espa??ol################################################
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA",NA,"requiere",
"oportunidades","aqui", "ofertas",
"horas", "importante","nuevo","id",
"sector","trabajo","personal","salario",
"nuevos","dos","requisition","id","contrato",
"años","ai","1", "2018", "cómo", "the", "lunes", "viernes"))
#Hacemos nuestra nube de palabras mas frecuentes, con Filtro
series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(word, sort = TRUE) %>%
with(wordcloud(unique(word), n,
max.words = 50,
random.order = F, colors = brewer.pal(name = "Dark2", n = 8)))
# contando palabras mas comunes en todo el texto de la serie, pero agrupados por
# terminos y CON FILTRO
series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras)  %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
# visualizacion de la frecuencia absoluta de palabras por terminos consultados
ungroup() %>%
mutate(book = factor(book, levels = titles),
text_order = nrow(.):1) %>%
ggplot(aes(reorder(word, text_order), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "Términos", y = "Frecuencia") +
coord_flip() +
theme(legend.position="none")
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA",NA,"requiere",
"oportunidades","aqui", "ofertas",
"horas", "importante","nuevo","id",
"sector","trabajo","personal","salario",
"nuevos","dos","requisition","id","contrato",
"años","ai","1", "2018", "cómo", "the", "lunes", "viernes",
"bus003507", "15", "30", "true", TRUE, "_relativeurls",
"	hbspt.cta", "_relativeurls", "hbspt.cta.load", "239039",
"muchas", "veces", "primera", "vez", "quieres", "saber"))
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
dplyr::select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word)%>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word)%>%
count(book, word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
slice(1:10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder_within(bigram, n, book), n, fill = book)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
scale_x_reordered() + xlab("bigramas") + labs(title ="Frecuencias de bigramas top 10")+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
=======
dolar$Fecha <- dmy(dolar$Fecha)
indice$Año.aaaa..Mes.mm. <- paste(indice$Año.aaaa..Mes.mm., '-01', sep = '')
indice$Año.aaaa..Mes.mm. <- ymd(indice$Año.aaaa..Mes.mm.)
format(presupuestos$fecha_inicio, format = '%Y')
format(presupuestos$fecha_terminacion, format = '%Y')
# 2004
x2004 <-  presupuestos[year(presupuestos$fecha_inicio) ==2004 ,]
x20041 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2004, 2])
x20042 <- mean(dolar[year(dolar$Fecha) == 2004, 2 ])
#2005
x2005 <- presupuestos[year(presupuestos$fecha_inicio) ==2005 ,]
x20051 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2005, 2])
x20052 <- mean(dolar[year(dolar$Fecha) == 2005, 2])
#2006
x2006 <- presupuestos[year(presupuestos$fecha_inicio) ==2006 ,]
x20061 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2006, 2])
x20062 <- mean(dolar[year(dolar$Fecha) == 2006, 2])
#2007
x2007 <- presupuestos[year(presupuestos$fecha_inicio) ==2007 ,]
x20071 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2007, 2])
x20072 <- mean(dolar[year(dolar$Fecha) == 2007, 2])
#2008
x2008 <- presupuestos[year(presupuestos$fecha_inicio) ==2008 ,]
x20081 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2008, 2])
x20082 <- mean(dolar[year(dolar$Fecha) == 2008,2])
x2009 <- presupuestos[year(presupuestos$fecha_inicio) ==2009 ,]
x20091 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2009, 2])
x20092 <- mean(dolar[year(dolar$Fecha) == 2009,2])
x2010 <- presupuestos[year(presupuestos$fecha_inicio) ==2010 ,]
x20101 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2010, 2])
x20102 <- mean(dolar[year(dolar$Fecha) == 2010,2])
x2013 <- presupuestos[year(presupuestos$fecha_inicio) ==2013 ,]
x20131 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2013, 2])
x20132 <- mean(dolar[year(dolar$Fecha) == 2013,2])
x2015 <- presupuestos[year(presupuestos$fecha_inicio) ==2015 ,]
x20151 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2015, 2])
x20152 <- mean(dolar[year(dolar$Fecha) == 2015,2])
y <- rbind(x2004[6] ,x2005[6] ,
x2006[6], x2007[6] ,
x2008[6] ,x2009[6] ,
x2010[6] ,x2013[6],
x2015[6])
View(y)
format(presupuestos$fecha_inicio, format = '%Y')
2009:2017
indice[year(indice$Año.aaaa..Mes.mm.) %in% 2009:2017, 2]
format(presupuestos$fecha_terminacion, format = '%Y')
format(presupuestos$fecha_inicio, format = '%Y')
format(presupuestos$fecha_terminacion, format = '%Y')
#2006
x2006 <- presupuestos[year(presupuestos$fecha_inicio) ==2006 ,]
x2006
# 2004
x2004 <-  presupuestos[year(presupuestos$fecha_inicio) ==2004 ,]
x20041 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2004:2005, 2])
x20042 <- mean(dolar[year(dolar$Fecha) %in% 2004:2005, 2 ])
#2005
x2005 <- presupuestos[year(presupuestos$fecha_inicio) ==2005 ,]
x20051 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) == 2005:2006, 2])
x20052 <- mean(dolar[year(dolar$Fecha) == 2005:2006, 2])
x20052 <- mean(dolar[year(dolar$Fecha) %in% 2005:2006, 2])
#2005
x2005 <- presupuestos[year(presupuestos$fecha_inicio) ==2005 ,]
x20051 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2005:2006, 2])
x20052 <- mean(dolar[year(dolar$Fecha) %in% 2005:2006, 2])
#2006
x2006 <- presupuestos[year(presupuestos$fecha_inicio) ==2006 ,]
x20061 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2006:2007, 2])
x20061.1 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2006:2008, 2])
x20062 <- mean(dolar[year(dolar$Fecha) %in% 2006:2007, 2])
x20062.1 <- mean(dolar[year(dolar$Fecha) %in% 2006:2008, 2])
format(presupuestos$fecha_inicio, format = '%Y')
format(presupuestos$fecha_terminacion, format = '%Y')
# 2004
x2004 <-  presupuestos[year(presupuestos$fecha_inicio) ==2004 ,]
x20041 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2004:2005, 2])
x20042 <- mean(dolar[year(dolar$Fecha) %in% 2004:2005, 2 ])
#2005
x2005 <- presupuestos[year(presupuestos$fecha_inicio) ==2005 ,]
x20051 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2005:2006, 2])
x20052 <- mean(dolar[year(dolar$Fecha) %in% 2005:2006, 2])
#2006
x2006 <- presupuestos[year(presupuestos$fecha_inicio) ==2006 ,]
x20061 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2006:2007, 2])
x20061.1 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2006:2008, 2])
x20062 <- mean(dolar[year(dolar$Fecha) %in% 2006:2007, 2])
x20062.1 <- mean(dolar[year(dolar$Fecha) %in% 2006:2008, 2])
#2007
x2007 <- presupuestos[year(presupuestos$fecha_inicio) ==2007 ,]
x20071 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2007:2008, 2])
x20072 <- mean(dolar[year(dolar$Fecha) %in% 2007:2008, 2])
#2008
x2008 <- presupuestos[year(presupuestos$fecha_inicio) ==2008 ,]
x20081 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2008:2009, 2])
x20082 <- mean(dolar[year(dolar$Fecha) %in% 2008:2009,2])
x2009 <- presupuestos[year(presupuestos$fecha_inicio) ==2009 ,]
x20091 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2009:2010, 2])
x20092 <- mean(dolar[year(dolar$Fecha) == 2009:2010,2])
x2009 <- presupuestos[year(presupuestos$fecha_inicio) ==2009 ,]
x20091 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2009:2010, 2])
x20092 <- mean(dolar[year(dolar$Fecha) %in% 2009:2010,2])
x2010 <- presupuestos[year(presupuestos$fecha_inicio) ==2010 ,]
x20101 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2010:2011, 2])
x20102 <- mean(dolar[year(dolar$Fecha) %in% 2010:2011,2])
x2013 <- presupuestos[year(presupuestos$fecha_inicio) ==2013 ,]
x20131 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2013:2014, 2])
x20131.1 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2013:2015, 2])
x20132 <- mean(dolar[year(dolar$Fecha) %in% 2013:2014,2])
x20132.1 <- mean(dolar[year(dolar$Fecha) %in% 2013:2015,2])
x2015 <- presupuestos[year(presupuestos$fecha_inicio) ==2015 ,]
x20151 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2015:2017, 2])
x20151.1 <- mean(indice[year(indice$Año.aaaa..Mes.mm.) %in% 2015:2019, 2])
x20152 <- mean(dolar[year(dolar$Fecha) %in% 2015:2017,2])
x20152.1 <- mean(dolar[year(dolar$Fecha) %in% 2015:2019,2])
format(presupuestos$fecha_inicio, format = '%Y')
IPS <- rbind(x20041, x20051, x20051, x20061,
x20061.1, x20071, x20081, x20081,
x20081, x20071, x20101, x20091,
x20131, x20131.1, x20151, x20151.1 )
dim(IPS)
dolar1 <- rbind(x20042, x20052, x20052, x20062,
x20062.1, x20072, x20082, x20082,
x20082, x20072, x20102, x20092,
x20132, x20132.1, x20152, x20152.1)
dim(dolar1)
base_final <- cbind(y, IPS, dolar1)
base_final
modelo <- lm(diff_presu ~ ., data = base_final)
summary(modelo)
plot(modelo)
residuals(modelo)
shapiro.test(residuals(modelo))
library(gamlss)
fitDist(base_final$diff_presu)
distri <- fitDist(base_final$diff_presu)
distri$family
modelo <- gamlss(formula = diff_presu ~ ., family = IG )
modelo <- gamlss(formula = diff_presu ~ ., family = IG,
data = base_final )
modelo
coef(modelo)
coef(modelo, "sigma")
modelo <- gamlss(formula = diff_presu ~ ., family = IG(link="log"),
data = base_final )
modelo <- gamlss(formula = diff_presu ~ ., family = inverse.gaussian(link="log"),
data = base_final )
modelo <- gamlss(formula = diff_presu ~ ., family = IG,
data = base_final, sigma.formula = ~. )
modelo
coef(modelo)
coef(modelo, "sigma")
hist(base_final$diff_presu)
hist(base_final$diff_presu)
lines(density(base_final$diff_presu))
histDist(base_final$diff_presu, family = IG)
histDist(base_final$diff_presu, family = IG,main = 'Distribución de la variable de respuesta')
histDist(base_final$diff_presu,
family = IG,
main = 'Distribución de la variable de respuesta',
xlab = 'Sobrecostos')
histDist(base_final$diff_presu,
family = IG,
main = 'Distribución de Y',
xlab = 'Sobrecostos')
hist(base_final$diff_presu)
histDist(base_final$diff_presu,
family = IG,
main = 'Distribución de Y',
xlab = 'Sobrecostos')
png(filename = 'distribucion.png')
histDist(base_final$diff_presu,
family = IG,
main = 'Distribución de Y',
xlab = 'Sobrecostos')
dev.off()
getwd
getwd()
summary(modelo)
term.plot(modelo, pages=1, ask=FALSE)
wp(modelo, ylim.all=.6)
wp(modelo)
png(filename = 'wp.png')
wp(modelo)
dev.off()
Rsq(modelo)
1 - pnorm(q = (225-210)/sqrt(10))
pnorm(q = (225-210)/sqrt(10))
pnorm(q = (225-210)/(10))
1-  pnorm(q = (225-210)/(10))
1-  pnorm(q = (225-210)/(10), mean = 0, sd = 1)
pnorm(q = (225-210)/(10), mean = 0, sd = 1, lower.tail = T)
pnorm(q = (225-210)/(10), mean = 0, sd = 1, lower.tail = F)
pnorm(q = (225-210)/(10), mean = 0, sd = 1, lower.tail = F) * 100
x <- c(-2, 3, 12, 4,-4)
sd(x)
var(x)
qnorm(0.95, mean = 5.5, sd = 0.5)
y <- c( 15, 20, 30, 17, 21, 28, 29, 40, 80)
mean(y)
sd(y)
qnorm(0.95)
qnorm(0.95) sqrt((0.22*(1-0.22))/100)
qnorm(0.95)* sqrt((0.22*(1-0.22))/100)
a <- qnorm(0.95)* sqrt((0.22*(1-0.22))/100)
0.22 - a
0.22 + a
qnorm(0.90)
((qnorm(0.90)^2)* (1.5^2))/0.5
z<- c(0.01 , 0.1 , 0.2 , 0.02, 0.1, 0.01, 0.05, 0.51)
sum(z)
x <- 1:7
z*x
x <- 0:7
z*x
sum(z*x)
(2 - 1.5)/sqrt(10)
library(rvest)
url_google <- 'https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos'
# Leyendo el código HTML de la pagina
google <- read_html(url_google)
google
url_cd<- 'https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos'
# Leyendo el código HTML de la pagina
cd <- read_html(url_cd)
urls_cd <- html_nodes(cd, '.iUh30')
urls_cd
urls_cd <- html_nodes(cd, '.iUh30')
urls_cd
num<-html_text(html_nodes(urls_cd, "td.u-align-right"))
provider<-html_text(html_nodes(table, "td.cell-provider-name"))
table<-html_table(urls_cd)[[2]]
table<-html_table(urls_cd)
table
html_table(urls_cd)
urls_cd <- html_nodes(cd, "iUh30")
table<-html_table(urls_cd)
table
html_nodes(cd, "iUh30")
urls_cd <- html_nodes(cd, "TbwUpd")
urls_cd
library(curl)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
library(curl)
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
library(XML)
library(XML)
install.packages("XML")
library(curl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
getURL(u)
library(curl)
library(XML)
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
library(RCurl)
get_google_page_urls <- function(u) {
# read in page contents
html <- CugetURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
get_google_page_urls <- function(u) {
# read in page contents
html <- getURL(u)
# parse HTML into tree structure
doc <- htmlParse(html)
# extract url nodes using XPath. Originally I had used "//a[@href][@class='l']" until the google code change.
attrs <- xpathApply(doc, "//h3//a[@href]", xmlAttrs)
# extract urls
links <- sapply(attrs, function(x) x[[1]])
# free doc from memory
free(doc)
# ensure urls start with "http" to avoid google references to the search page
links <- grep("http://", links, fixed = TRUE, value=TRUE)
return(links)
}
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
get_google_page_urls('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
read_html('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
a <- read_html('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
a
a[1]
a[[1]]
a <- html('https://www.google.com/search?client=firefox-b-d&q=ciencia+de+datos')
a
html_nodes(tech, "p")
html_nodes(a, "p")
html_nodes(a, "h1")
install.packages("quanteda")
# ---------------
require(quanteda)
archivo <- "Simposio/Mineria-texto/ciencia de datos.txt"
con <- file(archivo, open="r") # Abrimos la conexión
documento <- readLines(con)          # L
documento
archivo <- "Simposio/Mineria-texto/ciencia de datos.txt"
con <- file(archivo, open="r") # Abrimos la conexión
documento <- readLines(con)          # L
library(stringr)
regmatches(documento, gregexpr('^www.', documento))
regmatches(documento, gregexpr('www.', documento))
regmatches(documento, gregexpr('^www.', documento))
stringr::str_extract_all(documento , '^www.')
stringr::str_extract_all(documento , '^www')
stringr::str_extract_all(documento , 'www')
stringr::str_extract_all(documento , 'www\.')
stringr::str_extract_all(documento , 'www/.')
stringr::str_extract_all(documento , 'www/.[0-9a-z]+')
stringr::str_extract_all(documento , '^www\.[a-z0-9]+\.[a-z]+\.[a-z]*')
stringr::str_extract_all(documento , '^www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
stringr::str_extract_all(documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
str_extract_all(documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
str_extract_all(documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
url_cd<- 'https://www.google.com/search?q=ciencia+de+datos&client=firefox-b-d&gbv=1&sei=7MISXYqrAu7l_Qa1yLDgDw'
# Leyendo el código HTML de la pagina
cd <- read_html(url_cd)
library(rvest)
# Leyendo el código HTML de la pagina
cd <- read_html(url_cd)
urls_cd <- html_nodes(cd, "TbwUpd")
urls_cd
urls_cd <- html_nodes(cd, ".iUh30")
urls_cd
library(rvest)
library(rvest)
url_cd<- 'https://www.google.com/search?hl=eo&source=hp&ei=XuYSXeipOePx5gKktIKYAw&q=ciencia+de+datos&oq=ciencia+de+datos&gs_l=psy-ab.3..35i39i19j0i203l9.1982.3855..3957...0.0..0.316.3824.0j1j13j2......0....1..gws-wiz.....0..35i39j0i19.CK8eFUGN_ow
'
# Leyendo el código HTML de la pagina
cd <- read_html(url_cd)
url_cd<- 'https://www.google.com/search?hl=eo&source=hp&ei=XuYSXeipOePx5gKktIKYAw&q=ciencia+de+datos&oq=ciencia+de+datos&gs_l=psy-ab.3..35i39i19j0i203l9.1982.3855..3957...0.0..0.316.3824.0j1j13j2......0....1..gws-wiz.....0..35i39j0i19.CK8eFUGN_ow
'
# Leyendo el código HTML de la pagina
cd <- read_html(url_cd)
download.file(url_cd, destfile = "scrapedpage.html", quiet=TRUE)
# Leyendo el código HTML de la pagina
content <- read_html("scrapedpage.html")
content
urls_cd <- html_nodes(content, ".iUh30")
urls_cd
url_cd<- 'https://www.google.com/search?hl=eo&ei=Y-YSXcbiKObM5gLQqr_ICA&q=machine+learning&oq=machine+learning&gs_l=psy-ab.3..35i39i19l2j0i203l8.187034.190718..190864...2.0..0.254.3551.0j7j10......0....1..gws-wiz.......35i39j0i19j0i10i203j0i22i10i30j0i22i30.LdR-_xjpW8E'
url_cd
download.file(url_cd, destfile = "scrapedpage.html", quiet=TRUE)
# Leyendo el código HTML de la pagina
content <- read_html("scrapedpage.html")
# Leyendo el código HTML de la pagina
content <- read_html("scrapedpage.html")
content
urls_cd <- html_nodes(content, ".iUh30")
urls_cd
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
url_cd<- 'https://www.google.com/search?q=ciencia+de+datos&rlz=1C1GCEA_enCO836CO836&oq=ciencia+de+datos&aqs=chrome..69i57.3393j0j1&sourceid=chrome&ie=UTF-8'
# Leyendo el código HTML de la pagina
content <- read_html(url_cd)
urls_cd <- html_nodes(content, ".iUh30")
urls_cd
urls_cd <- html_nodes(content, ".TbwUpd")
urls_cd
urls_cd <- html_nodes(content, ".TbwUpd")
urls_cd
ciencia_datos <- '../Documents/Simposio/Mineria-texto/ciencia de datos.txt'
con <- file(ciencia_datos, open="r") # Abrimos la conexión
ciencia_datos <- '../Documents/Simposio/Mineria-texto/ciencia de datos.txt'
con <- file(ciencia_datos, open="r") # Abrimos la conexión
documento <- readLines(con)
documento
con
sapply(documento, str_extract_all,documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*'))
sapply(documento, str_extract_all,documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
library(stringr)
sapply(documento, str_extract_all,documento , 'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
sapply(documento, str_extract_all,string = documento ,pattern =   'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
class(documento)
documento
str_extract_all(string = documento ,pattern =   'www\\.[a-z0-9]+\\.[a-z]+\\.[a-z]*')
str_extract_all(string = documento ,pattern =   'https.+')
con <- file(ciencia_datos, open="r",  raw = FALSE) # Abrimos la conexión
documento <- readLines(con)
documento
con <- file(ciencia_datos, open="r",  raw = T) # Abrimos la conexión
documento <- readLines(con)
documento
documento[[1]]
documento[1]
sapply(documento, str_extract_all,string = documento ,pattern =   'https.+')
documento <- paste(readLines(con), collapse = ' ')
documento
con <- file(ciencia_datos, open="r",  raw = T) # Abrimos la conexión
documento <- paste(readLines(con), collapse = ' ')
documento
str_extract_all(string = documento ,pattern =   'https.+')
string = documento[[1]]
documento[[1]]
str_extract_all(string = documento[[1]] ,pattern =   'https.+')
str_extract_all(string = documento[[1]][1] ,pattern =   'https.+')
documento[[1]][1]
unclass(documento)
str_extract_all(string = unclass(documento) ,pattern = 'https.+')
unclass(documento)
documento
as.character(documento)
as.character(documento[1])
as.character(documento[[1]])
x <- '"Vínculos de accesibilidad Ir al contenido principal Ayuda sobre la accesibilidad Comentarios sobre la accesibilidad Google Cerca de 319,000,000 resultados (0.41 segundos)  Looking for results in English? Change to English Continuar usando español Configuración del idioma Anuncio      Experto en Ciencia de Datos | Minería de Datos | promidat.com?     Anunciowww.promidat.com/?     Matricúlate ya, 100% online, iniciamos el 8 de julio de 2019. Clases en Vivo. Foro de Ayuda 24/7...         Matricularse AquíFormulario De ContactoSolicitar MatrículaCertificaciones Y Títulos  Resultados de búsqueda Fragmento destacado de la Web Resultado de imagen para ciencia de datos www.youtube.com También se define La ciencia de datos como \"Un concepto para unificar estadísticas, análisis de datos, aprendizaje automático y sus métodos relacionados para comprender y analizar los fenómenos reales\",? empleando técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, la estadística, la ... Ciencia de datos - Wikipedia, la enciclopedia libre https://es.wikipedia.org/wiki/Ciencia_de_datos Comentarios Acerca de este resultado Resultados de la Web Ciencia de datos - Wikipedia, la enciclopedia libre https://es.wikipedia.org/wiki/Ciencia_de_datos  La ciencia de datos es un campo interdisciplinario que involucra métodos científicos, procesos y sistemas para extraer conocimiento o un mejor entendimiento ... ?Historia · ?Aplicaciones · ?Ciencia de datos y Big data · ?Científico de datos Ciencia de Datos | edX https://www.edx.org/es/aprende/ciencia-de-datos  Cursos en línea sobre Ciencia de datos (Data Science) y disciplinas relacionadas como la estadística, el aprendizaje automático, la inteligencia de negocios, ... Científico de datos: así es y así se forma uno en esta profesión cada ... https://www.xataka.com/otros/de-profesion-cientifico-de-datos  5 jun. 2019 - Por eso, hoy por hoy, podemos encontrar una gran diversidad de perfiles profesionales en el mundo de la ciencia de datos. Según Burtch ... ¿Qué diablos es Ciencia de Datos? – Ciencia y Datos – Medium https://medium.com/datos-y-ciencia/qué-diablos-es-ciencia-de-datos-f1c8c7add107  21 dic. 2018 - Aquí está mi intento más conciso: “La ciencia de datos es la disciplina de hacer que los datos sean útiles”. Siéntete libre de salir corriendo ... Ciencia de Datos Online Courses | Coursera https://es.coursera.org/browse/data-science  Enroll in Ciencia de Datos courses and Specializations for free. Los cursos y Programas Especializados de ciencia de datos enseñan los fundamentos para la ... ¿Qué es Ciencia de datos? - Definición en WhatIs.com https://searchdatacenter.techtarget.com/es/definicion/Ciencia-de-datos  La ciencia de datos es el estudio de dnde proviene la informacin qu representa y cmo se puede convertir en un recurso valioso para la creacin de estrate... Convocatoria para la formación de ciudadanos en ciencia de datos ... www.colciencias.gov.co › Convocatorias › Innovación Seleccionar 200 ciudadanos colombianos para acceder a formación teórico-práctica y certificación como Citizen Data Scientists para contribuir a la solución de ... Conoce el mundo de la Ciencia de Datos - Pragma SA https://www.pragma.com.co/academia/.../conoce-el-mundo-de-la-ciencia-de-datos  En el mundo de la Ciencia de Datos hablamos de big data, business intelligence, estadística, machine learning y otros conceptos técnicos que cada día surgen ... Definiendo “Ciencia de Datos” | SG Buzz - Software Guru https://sg.com.mx/revista/43/definiendo-ciencia-datos  Ahora todo es llamado “big data” y “ciencia de datos”. El problema es que no contamos con definiciones claras y comprobables.Se han realizado intentos para ... Ciencia de Datos y Analítica Avanzada - Perú | IBM https://www.ibm.com/co-es/analytics/data-science  Su equipo de ciencia de datos puede ayudar a varios departamentos, utilizando un conjunto diverso de herramientas y técnicas disponibles en IBM Data ... Búsquedas relacionadas con ciencia de datos  importancia de la ciencia de datos  ciencia de datos curso  aplicaciones de la ciencia de datos  ciencia de datos argentina  ciencia de datos con python  diplomado ciencia de datos  ciencia de datos y matemáticas  ciencia de datos ibm Navegación de páginas \t1\t 2 \t 3 \t 4 \t 5 \t 6 \t 7 \t 8 \t 9 \t 10 \t Siguiente Resultados adicionales Resultado del Gráfico de conocimiento Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Más imágenes Ciencia de datos Campo de estudio Descripción La ciencia de datos es un campo interdisciplinario que involucra métodos científicos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea ... Wikipedia También se buscó Ver 10 más Aprendizaje automático Aprendizaje automático Análisis informático Análisis informático Tecnología de la información Tecnología de la información Análisis de datos Análisis de datos Ciencias de la computación Ciencias de la computación Comentarios Vínculos a pie de página Colombia Medellín, Antioquia - De tu dirección de Internet - Usar la ubicación precisa  - Más información AyudaEnviar comentariosPrivacidadCondiciones"
> as.character(documento[[1]])
[1] "Vínculos de accesibilidad Ir al contenido principal Ayuda sobre la accesibilidad Comentarios sobre la accesibilidad Google Cerca de 319,000,000 resultados (0.41 segundos)  Looking for results in English? Change to English Continuar usando español Configuración del idioma Anuncio      Experto en Ciencia de Datos | Minería de Datos | promidat.com?     Anunciowww.promidat.com/?     Matricúlate ya, 100% online, iniciamos el 8 de julio de 2019. Clases en Vivo. Foro de Ayuda 24/7...         Matricularse AquíFormulario De ContactoSolicitar MatrículaCertificaciones Y Títulos  Resultados de búsqueda Fragmento destacado de la Web Resultado de imagen para ciencia de datos www.youtube.com También se define La ciencia de datos como \"Un concepto para unificar estadísticas, análisis de datos, aprendizaje automático y sus métodos relacionados para comprender y analizar los fenómenos reales\",? empleando técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, la estadística, la ... Ciencia de datos - Wikipedia, la enciclopedia libre https://es.wikipedia.org/wiki/Ciencia_de_datos Comentarios Acerca de este resultado Resultados de la Web Ciencia de datos - Wikipedia, la enciclopedia libre https://es.wikipedia.org/wiki/Ciencia_de_datos  La ciencia de datos es un campo interdisciplinario que involucra métodos científicos, procesos y sistemas para extraer conocimiento o un mejor entendimiento ... ?Historia · ?Aplicaciones · ?Ciencia de datos y Big data · ?Científico de datos Ciencia de Datos | edX https://www.edx.org/es/aprende/ciencia-de-datos  Cursos en línea sobre Ciencia de datos (Data Science) y disciplinas relacionadas como la estadística, el aprendizaje automático, la inteligencia de negocios, ... Científico de datos: así es y así se forma uno en esta profesión cada ... https://www.xataka.com/otros/de-profesion-cientifico-de-datos  5 jun. 2019 - Por eso, hoy por hoy, podemos encontrar una gran diversidad de perfiles profesionales en el mundo de la ciencia de datos. Según Burtch ... ¿Qué diablos es Ciencia de Datos? – Ciencia y Datos – Medium https://medium.com/datos-y-ciencia/qué-diablos-es-ciencia-de-datos-f1c8c7add107  21 dic. 2018 - Aquí está mi intento más conciso: “La ciencia de datos es la disciplina de hacer que los datos sean útiles”. Siéntete libre de salir corriendo ... Ciencia de Datos Online Courses | Coursera https://es.coursera.org/browse/data-science  Enroll in Ciencia de Datos courses and Specializations for free. Los cursos y Programas Especializados de ciencia de datos enseñan los fundamentos para la ... ¿Qué es Ciencia de datos? - Definición en WhatIs.com https://searchdatacenter.techtarget.com/es/definicion/Ciencia-de-datos  La ciencia de datos es el estudio de dnde proviene la informacin qu representa y cmo se puede convertir en un recurso valioso para la creacin de estrate... Convocatoria para la formación de ciudadanos en ciencia de datos ... www.colciencias.gov.co › Convocatorias › Innovación Seleccionar 200 ciudadanos colombianos para acceder a formación teórico-práctica y certificación como Citizen Data Scientists para contribuir a la solución de ... Conoce el mundo de la Ciencia de Datos - Pragma SA https://www.pragma.com.co/academia/.../conoce-el-mundo-de-la-ciencia-de-datos  En el mundo de la Ciencia de Datos hablamos de big data, business intelligence, estadística, machine learning y otros conceptos técnicos que cada día surgen ... Definiendo “Ciencia de Datos” | SG Buzz - Software Guru https://sg.com.mx/revista/43/definiendo-ciencia-datos  Ahora todo es llamado “big data” y “ciencia de datos”. El problema es que no contamos con definiciones claras y comprobables.Se han realizado intentos para ... Ciencia de Datos y Analítica Avanzada - Perú | IBM https://www.ibm.com/co-es/analytics/data-science  Su equipo de ciencia de datos puede ayudar a varios departamentos, utilizando un conjunto diverso de herramientas y técnicas disponibles en IBM Data ... Búsquedas relacionadas con ciencia de datos  importancia de la ciencia de datos  ciencia de datos curso  aplicaciones de la ciencia de datos  ciencia de datos argentina  ciencia de datos con python  diplomado ciencia de datos  ciencia de datos y matemáticas  ciencia de datos ibm Navegación de páginas \t1\t 2 \t 3 \t 4 \t 5 \t 6 \t 7 \t 8 \t 9 \t 10 \t Siguiente Resultados adicionales Resultado del Gráfico de conocimiento Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Resultado de imagen para ciencia de datos Más imágenes Ciencia de datos Campo de estudio Descripción La ciencia de datos es un campo interdisciplinario que involucra métodos científicos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea ... Wikipedia También se buscó Ver 10 más Aprendizaje automático Aprendizaje automático Análisis informático Análisis informático Tecnología de la información Tecnología de la información Análisis de datos Análisis de datos Ciencias de la computación Ciencias de la computación Comentarios Vínculos a pie de página Colombia Medellín, Antioquia - De tu dirección de Internet - Usar la ubicación precisa  - Más información AyudaEnviar comentariosPrivacidadCondiciones"'
x <- as.character(documento[1])
x <- as.character(documento[1])
x
y <- 'prueba'
y
str_extract_all( x ,'https.+')
regmatches(x, regexpr("https.+", x))
regexpr("https.+", x)
Func_1<-function(x){
x[1]^2
}
resultado<-optimize(Func_1,c(-5,5),maximum=FALSE,tol=1e-8)
#Vemos los resultados
print(resultado$minimum)
print(resultado$objective)
#Generamos el rango de valores para x y y=f(x)
x<-seq(-5,5,length.out=100)
y<-Func_1(expand.grid(x))
plot(x,y,xlab="x",ylab="f(x)",col="4")
#Generamos el rango de valores para x y y=f(x)
x<-seq(-5,5,length.out=100)
y<-Func_1(expand.grid(x))
dim(x)
length(x)
length(y)
#Vemos los resultados
print(resultado$minimum)
print(resultado$objective)
#Generamos el rango de valores para x y y=f(x)
x<-seq(-5,5,length.out=100)
x
y<-Func_1(expand.grid(x))
y
class(y)
class(x)
as.numeric(y)
as.data.frame(x)
#Generamos el rango de valores para x y y=f(x)
x<-as.data.frame(seq(-5,5,length.out=100))
y<-Func_1(expand.grid(x))
plot(x,y,xlab="x",ylab="f(x)",col="4")
x
#Generamos el rango de valores para x y y=f(x)
x<-seq(-5,5,length.out=100)
y<-Func_1(expand.grid(x))
x <- as.data.frame(x)
plot(x,y,xlab="x",ylab="f(x)",col="4")
plot(x$x,y$Var1,xlab="x",ylab="f(x)",col="4")
points(resultado$minimum,resultado$objective, col="red",pch=19)
rect(resultado$minimum-0.3, resultado$objective-0.7, resultado$minimum+0.3,resultado$objective+0.7)
library(readr)
setwd('../Documents/Simposio/Mineria-texto/')
>>>>>>> 69aaf85c3d6307fa6583f1cafd007c14a95f1259
