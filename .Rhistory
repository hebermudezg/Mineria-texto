# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word)%>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word)%>%
count(book, word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
slice(1:10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder_within(bigram, n, book), n, fill = book)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
scale_x_reordered() + xlab("bigramas") + labs(title ="Frecuencias de bigramas top 10")+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
series
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
frequency
# calculate percent of word use across all novels
potter_pct <- series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(word) %>%
transmute(word, all_words = n / sum(n))
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
# calculate percent of word use within each novel
frequency <- series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(book, word) %>%
mutate(book_words = n / sum(n)) %>%
left_join(potter_pct) %>%
arrange(desc(book_words)) %>%
ungroup()
frequency
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
win.graph()
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
win.graph()
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Harry Potter Series", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
ggplot(frequency, aes(x = book_words, y = all_words, color = abs(all_words - book_words))) +
geom_abline(color = "red", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~ book, ncol = 2) +
theme(legend.position="none") +
labs(y = "Términos y temas consultados ", x = NULL)
frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words),
p_value = cor.test(book_words, all_words)$p.value)
frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words))
core <- frequency %>%
group_by(book) %>%
summarize(correlation = cor(book_words, all_words))
library(Kableextra)
library(KablExtra)
library(kablExtra)
library(kableExtra)
kable(core)
kable(core, "markdown")
core <- frequency %>%
group_by(book) %>%
summarize(correlacion = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(term =book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
core <- frequency %>%
group_by(término =book) %>%
summarize(correlación = cor(book_words, all_words))
library(kableExtra)
kable(core, "markdown")
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
library(igraph)
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
library(ggplot2) #
library(dplyr)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(stringr)
library(tm)
library(forcats)
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(estadistica),
text = estadistica) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
#correlacion
word_cor %>%
filter(item1 == "estadística") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(big_data),
text = big_data) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
#correlacion
word_cor %>%
filter(item1 == "big") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#analitica de datos
(ps_words <- tibble(chapter = seq_along(Analitica_de_datos),
text = Analitica_de_datos) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
word_cor %>%
filter(item1 == "analítica") %>%
arrange(desc(correlation))
#vizualizacion
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
# Toma una terminologia
(ps_words <- tibble(chapter = seq_along(machine_learning),
text = machine_learning) %>%
unnest_tokens(word, text) %>%
filter(!word %in% stop_words_spanish$word) %>%
filter(!word %in% mas_palabras$word))
# determina scuales on las palabras correlacionadas más altas que aparecen
# con alguna palabra en especial
word_cor %>%
filter(item1 == "machine") %>%
arrange(desc(correlation))
#ggraph es para visualizar bigrams,
#podemos usarlo para visualizar las correlaciones dentro de grupos de palabra
ps_words %>%
group_by(word) %>%
filter(n() >= 10) %>%
pairwise_cor(word, chapter) %>%
filter(!is.na(correlation),
correlation > .1) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
new_x <- paste(x, within, sep = sep)
stats::reorder(new_x, by, FUN = fun)
}
#' @rdname reorder_within
#' @export
scale_x_reordered <- function(..., sep = "___") {
reg <- paste0(sep, ".+$")
ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
#' @rdname reorder_within
#' @export
scale_y_reordered <- function(..., sep = "___") {
reg <- paste0(sep, ".+$")
ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}
######## para un ejemplo #############
hh <- as.matrix(ofertas_ciencia_de_datos[1])
removeWords(hh,stopwords("spanish"))
removePunctuation(hh)
removeNumbers(hh)
stripWhitespace(hh)
hh <- tibble(hh)
hh %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
library(ggplot2) #
library(dplyr)
library(tidytext)
library(tidyverse)
library(wordcloud)
library(stringr)
library(tm)
library(forcats)
# Cargar terminologias
load(file = "textominado.RData")
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
## contando palabras mas comunes en todo el texto de la serie SIN FILTRO
series %>%
count(word, sort = TRUE)
#### Definimos las stop words en espa??ol################################################
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA",NA,"requiere",
"oportunidades","aqui", "ofertas",
"horas", "importante","nuevo","id",
"sector","trabajo","personal","salario",
"nuevos","dos","requisition","id","contrato",
"años","ai","1", "2018", "cómo", "the", "lunes", "viernes"))
#Hacemos nuestra nube de palabras mas frecuentes, con Filtro
series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras) %>%
count(word, sort = TRUE) %>%
with(wordcloud(unique(word), n,
max.words = 50,
random.order = F, colors = brewer.pal(name = "Dark2", n = 8)))
# contando palabras mas comunes en todo el texto de la serie, pero agrupados por
# terminos y CON FILTRO
series %>%
anti_join(stop_words_spanish) %>%
anti_join(mas_palabras)  %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
# visualizacion de la frecuencia absoluta de palabras por terminos consultados
ungroup() %>%
mutate(book = factor(book, levels = titles),
text_order = nrow(.):1) %>%
ggplot(aes(reorder(word, text_order), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "Términos", y = "Frecuencia") +
coord_flip() +
theme(legend.position="none")
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA",NA,"requiere",
"oportunidades","aqui", "ofertas",
"horas", "importante","nuevo","id",
"sector","trabajo","personal","salario",
"nuevos","dos","requisition","id","contrato",
"años","ai","1", "2018", "cómo", "the", "lunes", "viernes",
"bus003507", "15", "30", "true", TRUE, "_relativeurls",
"	hbspt.cta", "_relativeurls", "hbspt.cta.load", "239039",
"muchas", "veces", "primera", "vez", "quieres", "saber"))
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
dplyr::select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word)%>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word)%>%
count(book, word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
slice(1:10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder_within(bigram, n, book), n, fill = book)) +
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
scale_x_reordered() + xlab("bigramas") + labs(title ="Frecuencias de bigramas top 10")+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
