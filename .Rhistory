for (i in 1:length(enlaces)){
leer_html <- read_html(enlaces[i])
elempleo[i] <- leer_html %>% html_nodes(".description-block span") %>% html_text()
}
url <- "https://www.opcionempleo.com.co/empleo-mineria-de-datos.html"
enlaces <- read_html(url) %>%  html_nodes(".job a") %>%  xml_attr("href")
enlaces <- gsub(pattern = "/job/", "",enlaces)
enlaces <- paste("https://www.opcionempleo.com.co/jobview/", enlaces, sep = "")
opcionempleo <- vector()
for (i in seq(1,length(enlaces),2)){
leer_html <- read_html(enlaces[i])
opcionempleo[i] <- leer_html %>% html_nodes(".advertise_compact") %>% html_text()
}
url <- "https://co.linkedin.com/jobs/search?countryRedirected=1&pageNum=0&position=1&keywords=Cient%C3%ADfico%20de%20datos&location=Colombia&currentJobId=1293908240"
enlaces<-read_html(url) %>% html_nodes(".jobs-search-result-item a") %>% html_attr("href")
linkedinES <- vector()
for (i in 1:length(enlaces)){
leer_html <- read_html(enlaces[i])
linkedinES[i] <- leer_html %>% html_nodes(".description__text--rich") %>% html_text()
}
# uniedo todas las cadenas de texto recolectadas sobre "ofertas de empleo ciencia de datos"
ofertas_ciencia_de_datos <- c(elempleo, opcionempleo, linkedinES) # 68
url <- "https://www.elempleo.com/co/ofertas-empleo/trabajo-estadistica/"
enlaces <- read_html(url) %>%  html_nodes(".area-bind") %>%  xml_attr("data-url")
enlaces<- paste("https://www.elempleo.com",enlaces, sep = "")
elempleoEST <- vector()
for (i in seq(1,length(enlaces),2)){
leer_html <- read_html(enlaces[i])
elempleoEST[i] <- leer_html %>% html_nodes(".description-block span") %>% html_text()
}
# Opcion empleo EST
url<-"https://www.opcionempleo.com.co/empleo-estadistico.html"
enlaces<-read_html(url) %>% html_nodes(".clickable a") %>% html_attr("href")
enlaces<- paste("https://www.opcionempleo.com.co",enlaces,sep = "")
opcionempleoEST <- vector()
for (i in 1:length(enlaces)){
leer_html <- read_html(enlaces[i])
opcionempleoEST[i] <- leer_html %>% html_nodes(".advertise_compact") %>% html_text()
}
#Jobisjob EST
url <- "https://www.jobisjob.com.co/estadistica/trabajos"
enlaces <- read_html(url) %>%  html_nodes(".offer a") %>%  xml_attr("href")
enlaces<- enlaces[-c(2,5,9,11,13,15)]
jobisjobEST <- vector()
for (i in 1:length(enlaces)){
leer_html <- read_html(enlaces[i])
jobisjobEST[i] <- leer_html %>% html_nodes(".description") %>% html_text()
}
#Jooble EST
url<- "https://co.jooble.org/trabajo-profesional-estadistica"
enlaces<-read_html(url) %>% html_nodes(".paddings a") %>% html_attr("href")
#con expresiones regulares eliminamos los enlaces que contengan company
patron <- '(company)'
trash<-c(grep(pattern = patron, enlaces) )
enlaces<-enlaces[-(trash)]
joobleEST <- vector()
for (i in 1:length(enlaces)){
leer_html <- read_html(enlaces[i])
joobleEST[i] <- leer_html %>% html_nodes(".desc_text_paragraph p")%>% html_text()
}
#Unificando las descripciones de las ofertas de empleo de estadistica
Ofertas_estadistica<- c(elempleoEST,opcionempleoEST,jobisjobEST,joobleEST)
url <- "https://cleverdata.io/que-es-machine-learning-big-data/"
ML1 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://www.sas.com/es_co/insights/analytics/machine-learning.html"
ML2 <- read_html(url) %>% html_nodes(".cq-colctrl-lt0 , #machine-learning-usersscroll p , h4 , h4 .txt-white , #machine-learning-importancescroll p , h2+ p , p .txt-large") %>% html_text()
url <- "https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico"
ML3 <- read_html(url) %>% html_nodes("ul+ p , p:nth-child(7) , h3+ p , p+ p , p:nth-child(63) , li li+ li , .mw-redirect , p:nth-child(71) , dd , p:nth-child(1)") %>% html_text()
url <- "https://blog.adext.com/machine-learning-guia-completa/"
ML4 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://aws.amazon.com/es/machine-learning/"
ML5 <- read_html(url) %>% html_nodes("p, b") %>% html_text()
url <- "https://retina.elpais.com/retina/2017/10/19/innovacion/1508392516_816211.html"
ML6 <- read_html(url) %>% html_nodes("p , .articulo-subtitulo") %>% html_text()
# uniendo la infomacion sobre "machine learning"
machine_learning <-c(ML1,ML2,ML3, ML4, ML5, ML6)
url <- "https://www.xataka.com/otros/de-profesion-cientifico-de-datos"
CD1 <- read_html(url) %>% html_nodes("p:nth-child(52) , p:nth-child(51) , .js-post-images-container li , .article-asset-normal+ p") %>% html_text()
url <- "https://medium.com/datos-y-ciencia/qu%C3%A9-diablos-es-ciencia-de-datos-f1c8c7add107"
CD2 <- read_html(url) %>% html_nodes(".sectionLayout--fullWidth+ .sectionLayout--insetColumn , #58ff") %>% html_text()
url <- "https://es.wikipedia.org/wiki/Ciencia_de_datos"
CD3 <- read_html(url) %>% html_nodes("p+ ol li , p+ ul li , p") %>% html_text()
url <- "https://searchdatacenter.techtarget.com/es/definicion/Ciencia-de-datos"
CD4 <-  read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://www.pragma.com.co/academia/conceptos/conoce-el-mundo-de-la-ciencia-de-datos"
CD5 <-  read_html(url) %>% html_nodes("p") %>% html_text()
# uniendo la informacion sobre "ciencia de datos"
ciencia_de_datos <- c(CD1, CD2, CD3, CD4, CD5)
url <- "https://es.wikipedia.org/wiki/Estad%C3%ADstica"
E1 <- read_html(url) %>% html_nodes("td li , h3+ ul li , ul+ ul li , p+ ul li , p") %>% html_text()
url <- "https://economipedia.com/definiciones/estadistica.html"
E2 <- read_html(url) %>% html_nodes(".entry-content li , .entry-content p , #main strong") %>% html_text()
url <- "https://www.significados.com/estadistica/"
E3 <- read_html(url) %>% html_nodes("p:nth-child(4) , h3+ p , h2+ p , .desktop-only+ p") %>% html_text()
url <- "https://www.monografias.com/trabajos57/aprendizaje-estadistico/aprendizaje-estadistico.shtml"
E4 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://economipedia.com/definiciones/estadistica.html"
E5 <- read_html(url) %>% html_nodes("p:nth-child(4) , h3+ p , h2+ p , .desktop-only+ p") %>% html_text()
#uniendo infomacion sobre "estadistica"
estadistica <- c(E1, E2, E3, E4, E5)
url <- "https://es.wikipedia.org/wiki/Inteligencia_artificial"
IA1 <- read_html(url) %>% html_nodes("ul+ ul li , p+ ul li , p") %>% html_text()
url <- "https://searchdatacenter.techtarget.com/es/definicion/Inteligencia-artificial-o-AI"
IA2 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://www.muyinteresante.es/tecnologia/articulo/ventajas-y-riesgos-de-la-inteligencia-artificial-651449483429"
IA3 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://www.iberdrola.com/te-interesa/tecnologia/que-es-inteligencia-artificial"
IA4 <- read_html(url) %>% html_nodes(".container") %>% html_text()
url <- "http://www.fgcsic.es/lychnos/es_es/articulos/inteligencia_artificial"
IA5 <- read_html(url) %>% html_nodes(".texto") %>% html_text()
# uniendo informacion sobre IA
inteligencia_artificial <- c(IA1, IA2, IA3, IA4, IA5)
url <- "https://www.sas.com/es_co/insights/big-data/what-is-big-data.html"
BD1 <- read_html(url) %>% html_nodes("#dmusersscroll p , h4 , .list-bullet , .cq-colctrl-lt8-c1 p , #dmhistoryscroll div , p .txt-large") %>% html_text()
url <- "https://www.oracle.com/co/big-data/guide/what-is-big-data.html"
BD2 <- read_html(url) %>% html_nodes(".c89w1 p , #cw31panel-0_0 p , .c81v0 li , .c81w1 p") %>% html_text()
url <- "https://www.bit.es/knowledge-center/que-es-big-data-introduccion-a-big-data/"
BD3 <- read_html(url) %>% html_nodes(".col-md-8 main") %>% html_text()
url <- "https://www.powerdata.es/big-data"
BD4 <- read_html(url) %>% html_nodes(".row-fluid") %>% html_text()
url <- "https://es.wikipedia.org/wiki/Macrodatos"
BD5 <- read_html(url) %>% html_nodes("#content") %>% html_text()
# uniendo infomacion "big data"
big_data <- c(BD1, BD2, BD3, BD4, BD5)
url <- "https://www.analiticanegocios.com/analitica-de-datos/"
AD1 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://www.kienyke.com/tendencias/tecnologia/analitica-de-datos-que-es-y-como-vamos-en-el-tema-de-colombia"
AD2 <-  read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://searchdatacenter.techtarget.com/es/guia/Principios-de-la-analitica-de-datos-una-guia-esencial"
AD3 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://ticnegocios.camaravalencia.com/servicios/tendencias/los-efectos-positivos-de-la-analitica-de-datos-en-la-empresa/"
AD4 <- read_html(url) %>% html_nodes("p") %>% html_text()
url <- "https://blog.es.logicalis.com/analytics/la-anal%C3%ADtica-de-datos-en-el-coraz%C3%B3n-de-la-transformaci%C3%B3n-digital"
AD5 <- read_html(url) %>% html_nodes("p") %>% html_text()
Analitica_de_datos <- c(AD1, AD2, AD3, AD4, AD5)
save(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,ciencia_de_datos,
estadistica,inteligencia_artificial, big_data, Analitica_de_datos,
file = "textominado.RData")
load(file = "textominado.RData")
library(tidytext) #
library(tm) #
library(wordcloud)
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_empleo, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
## limpiemos los datos antes
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
class(series)
series %>%
count(word, sort = TRUE)
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- series %>% anti_join(mas_palabras)
# nube de palabras de 1-grama
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words = 50,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
# nube de palabras de 1-grama
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words = 50,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
# nube de palabras de 1-grama
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words = 50,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
# nube de palabras de 1-grama
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words =60,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
win.graph()
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words =60,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
series %>%group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10)
series %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(reorder(word, n), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "", y = "Frecuencia", title = "Frecuencia de palabras") +
coord_flip() + theme(legend.position="none")
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(reorder(word, n), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "", y = "Frecuencia", title = "Frecuencia de palabras") +
coord_flip() + theme(legend.position="none")
# visualizacion de la frecuencia absoluta de palabras por términos consultados
series %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(reorder(word, n), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "", y = "Frecuencia", title = "Frecuencia de palabras") +
coord_flip() + theme(legend.position="none")
series %>%group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10)
library(tidytext) #
library(tm) #
library(wordcloud)#
# creando marco de datos
titles <- c("ofertas ciencia de datos", "ofertas estadística", "machine learning",
"ciencia de datos", "estadística",
"inteligencia artificial", "big data", "analítica")
books <- list(ofertas_ciencia_de_datos, Ofertas_estadistica, machine_learning,
ciencia_de_datos, estadistica,
inteligencia_artificial, big_data, Analitica_de_datos)
# se recopilaron 1215 cadenas de texto (todos los temas)
## limpiemos los datos antes
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(word, text) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))
series
class(series)
## contando frecuencias ------------------------
series %>%
count(word, sort = TRUE)
## eliminado stopwords -------------------------------
## haciendo nube de palabras
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- series %>% anti_join(mas_palabras)
# nube de palabras de 1-grama
#win.graph()
series %>%
count(word, sort = TRUE) %>% with(wordcloud(unique(word), n, max.words =60,
random.order = F,colors = brewer.pal(name = "Dark2", n = 8)))
# conteo agrupando por término
series %>%group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10)
####### palabra individual
# Podemos visualizar esto con
# visualizacion de la frecuencia absoluta de palabras por términos consultados
series %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(reorder(word, n), n, fill = book)) +
geom_bar(stat = "identity") +
facet_wrap(~ book, scales = "free_y") +
labs(x = "", y = "Frecuencia", title = "Frecuencia de palabras") +
coord_flip() + theme(legend.position="none")
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series
stop_words_spanish$word <- as.character(stop_words_spanish$word)
mas_palabras$word <- as.character(mas_palabras$word)
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- series %>% anti_join(mas_palabras)
series
stop_words_spanish <- data.frame(bigram = stopwords("spanish"))
mas_palabras <- data.frame(bigram = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- series %>% anti_join(mas_palabras)
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
series <- tibble()
for(i in seq_along(titles)) {
clean <- tibble(chapter = seq_along(books[[i]]),
text = books[[i]]) %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = titles[i]) %>%
select(book, everything())
series <- rbind(series, clean)
}
# convertir titulos a factores
series$book <- factor(series$book, levels = rev(titles))
series
stop_words_spanish <- data.frame(bigram = stopwords("spanish"))
mas_palabras <- data.frame(bigram = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos", "en", "el", "la"))
series
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos", "en", "el", "la"))
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
stop_words_spanish <- data.frame(word = stopwords("spanish"))
mas_palabras <- data.frame(word = c("tener", "cada", "ser", "así", "hacer", "si",
"uso", "debe", "tipo", "años", "pueden", "puede",
"si", "sí", "NA", "NA NA", "1", "2018", "inglés",
"the", "cómo", "dos", "en", "el", "la", "na"))
# quitnado stopwors y mas palabras
series <- series %>% anti_join(stop_words_spanish)
series <- series %>% anti_join(mas_palabras)
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>% filter(bigram != "NA NA") %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
series %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words_spanish$word,
!word2 %in% stop_words_spanish$word) %>%
filter(!word1 %in% mas_palabras$word,
!word2 %in% mas_palabras$word) %>%
count(book,word1, word2, sort = TRUE) %>%
unite("bigram", c(word1, word2), sep = " ") %>%
group_by(book) %>%
top_n(8) %>%
ungroup() %>% filter(bigram != "NA NA") %>%
mutate(book = factor(book) %>% forcats::fct_rev()) %>%
ggplot(aes(reorder(bigram,n), n, fill = book))+
geom_bar(stat = "identity", alpha = .8, show.legend = FALSE)+
facet_wrap(~ book, ncol = 2, scales = "free") +
coord_flip()
